{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slz520f/github-basic-kadai/blob/main/master_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 自然言語処理の概要"
      ],
      "metadata": {
        "id": "KQIid2iJOzSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 教材要約（文語調、箇条書き）\n",
        "\n",
        "1. **目標**:\n",
        "   - 自然言語処理（NLP）の技術理解。\n",
        "   - NLPの用途と利用シーンの理解。\n",
        "   - NLPに使われるライブラリや学習モデルの理解。\n",
        "\n",
        "2. **自然言語処理の定義**:\n",
        "   - 人間の言語をコンピュータで分析、処理する技術。\n",
        "   - 日常言葉のコンピュータによる分析を含む。\n",
        "\n",
        "3. **用途と利用シーン**:\n",
        "   - 感情分析、文章要約。\n",
        "   - 機械翻訳、スマートスピーカー、チャットボット。\n",
        "   - 検索エンジン、迷惑メールフィルター。\n",
        "\n",
        "4. **代表的なライブラリとその機能**:\n",
        "   - re、Beautiful Soup、Gensim、NumPy。\n",
        "   - MeCab、Scikit-learn、wordcloud、spaCy、Transformers。\n",
        "\n",
        "5. **代表的な学習モデル**:\n",
        "   - BERT（文章分類、感情分析に使用）。\n",
        "   - GPT-2（文章生成に使用）。\n",
        "\n",
        "### 明解な要約文章（文語調）\n",
        "\n",
        ">本章では、自然言語処理（NLP）の基礎から応用までを学びます。NLPは人間の言葉をコンピュータで分析、処理する技術で、感情分析や文章要約など多様な用途で活用されています。また、機械翻訳、スマートスピーカー、チャットボット、検索エンジン、迷惑メールフィルターなどの日常技術にも深く関わっています。この章では、NLPにおける代表的なライブラリと学習モデル（BERT、GPT-2など）の機能と用途についても学びます。\n",
        "\n",
        "次に、残りの２つのベクトルでの要約を進めます。それぞれ「極めてわかりやすく抽象化された口語調の要約」と「教材内の例文コードを全て網羅し、箇条書きの説明を加えたコード集」になります。まずは、口語調の要約から行いますね。\n",
        "\n",
        "### 教材要約（口語調、抽象化）\n",
        "\n",
        ">自然言語処理とは簡単に言うと、私たちが普段使ってる言葉をコンピュータが理解して処理すること。例えば、文章を短くまとめたり、ポジティブかネガティブかを判断したりするのも自然言語処理のおかげ。\n",
        "\n",
        "他にも、Google翻訳みたいなのや、アレクサみたいなスマートスピーカー、LINEでよく見るチャットボットも実は自然言語処理が使われている。検索エンジンや迷惑メールフィルターにも使われていて、日常のいろんなところで活躍している。\n",
        "\n",
        "この教材では、自然言語処理でよく使われるプログラムのこと（ライブラリ）や、文章を分類したり作ったりするのに使うBERTやGPT-2っていう学習モデルについても学べる。\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "xLOVupd2Ovs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##代表的な自然言語処理ライブラリ\n",
        "\n",
        "\n",
        "| Library        | Learning Content                                                                                         | |\n",
        "|----------------|----------------------------------------------------------------------------------------------------------|---------|\n",
        "| re             | 正規表現モジュールと呼ばれており、自然言語処理では前処理で使用されています。                                       |\n",
        "| Beautiful Soup | 主にスクレイピングする際に使用します。自然言語処理ではインターネットからデータを取得する際に利用されます。                       |\n",
        "| Gensim         | Word2Vecと呼ばれる分散表現の手法があり、この手法はGensimから使用できます。                                         |       |\n",
        "| NumPy          | 多次元配列を扱う際に使用されるライブラリです。今回はcos類似度を計算するために使用します。                                  |  \n",
        "| MeCab          | 形態素解析と呼ばれる自然言語処理技術の1種で、MeCabは形態素解析エンジンの1つです。他にも形態素解析エンジンはありますが、今回は代表的なものを学習します。 |\n",
        "| Scikit-learn   | Scikit-learnは機械学習のライブラリとして有名です。今回はTF-IDFと呼ばれる自然言語処理技術の1つを利用するために使います。           |\n",
        "| wordcloud      | ワードクラウドと呼ばれる形態素解析を用いる際に一緒に使用されることが多い自然言語処理の手法の1つです。                               |\n",
        "| spaCy          | 固有表現抽出と呼ばれる自然言語処理技術の一つで、その内の一つのspaCyというライブラリを使用できます。                            |\n",
        "| Transformers   | huggingfaceが公開している機械学習のライブラリで、特に自然言語処理に特化したライブラリです。今回はBERT、GPT-2のモデルを用いたり、文章分類、感情分析を使用したりするときに使用します。 |\n"
      ],
      "metadata": {
        "id": "xFGvVOxYTzQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# テキストの前処理を学ぼう"
      ],
      "metadata": {
        "id": "hmlcMqpbQloM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **目標**:\n",
        "  - テキストの前処理技術を理解する。\n",
        "  - 前処理の用途を理解する。\n",
        "  - 前処理に使用されるライブラリや手法の使い方を学ぶ。\n",
        "\n",
        "- **テキストの前処理とは**:\n",
        "  - テキストを機械が処理できる形式に変換する工程。\n",
        "  - クリーニング、単語分割、正規化、ストップワード除去を含む。\n",
        "\n",
        "- **正規表現**:\n",
        "  - 文字列パターンを表す記法。\n",
        "  - Pythonでは`re`モジュールで使用。\n",
        "\n",
        "- **スクレイピングとBeautiful Soup4**:\n",
        "  - Webデータを自動抽出する技術。\n",
        "  - Pythonで利用可能なスクレイピングライブラリ。\n",
        "\n",
        "- **ストップワード**:\n",
        "  - 分析に不要な単語を除外すること。\n",
        "\n",
        ">本章では、自然言語処理の基本であるテキストの前処理に焦点を当てます。前処理には、テキストをクリーニングし、単語を分割し、正規化し、そして不要な単語（ストップワード）を除去する工程が含まれます。また、正規表現を用いて不要な文字列を効率的に取り除く方法、スクレイピング技術でWebからデータを収集する方法、そしてPythonの`Beautiful Soup4`ライブラリの使用方法についても学びます。これらの技術は自然言語処理の基礎であり、精度の高いAIモデル構築のために不可欠です。\n",
        "\n",
        "---\n",
        "\n",
        ">自然言語処理でテキストを扱う前にいくつか準備することはまず、テキストをキレイにすること。余計な文字や記号を取りはらって、テキストだけにする。それから、特に日本語の場合、単語をばらばらにすることも大事。そして単語をきれいな形に揃えて、意味のない小さい単語（ストップワードっていう）を取り除きます。\n",
        "\n",
        ">正規表現っていう特殊な記法を使って、テキストの中の特定のパターンを見つけて、それを使ってテキストを整理します。Pythonには`re`っていうモジュールがあって、これがとても便利。\n",
        "\n",
        ">それと、Webから情報を集めるスクレイピングっという技術もあって、これを使えば大量のデータをサクッと取ってこれる。Pythonの`Beautiful Soup4`ってライブラリを使うと、Webページから必要な部分だけを抜き出して、自分のデータにできる。\n",
        "\n",
        ">この章で学んだのは、こういったテキストの準備作業。これができれば、AIがテキストをもっと上手に理解できるようになる。\n",
        "\n",
        "---\n",
        "\n",
        "最後に、「教材内の例文コードを全て網羅し、箇条書きの説明を加えたコード集」の要約に進みます。この部分では、教材内のコード例を利用して、それぞれのコードの役割や使用方法について説明します。\n",
        "\n",
        "#### 正規表現モジュール`re`の使い方\n",
        "- **文字列の検索**:\n",
        "  - `re.match`: 文字列の始めから正規表現に合うかチェック。\n",
        "  - `re.search`: 文字列全体から正規表現に合う部分を探す。\n",
        "  - `re.findall`: 文字列全体から正規表現に合うすべての部分をリストで返す。\n",
        "\n",
        "- **文字列の置換**:\n",
        "  - `re.sub`: 正規表現に合う部分を別の文字列で置換。\n",
        "\n",
        "- **テキストクリーニングの例**:\n",
        "  - 不要な文字（記号、空白など）を正規表現で除去。\n",
        "\n",
        "#### スクレイピングと`Beautiful Soup4`の使用\n",
        "- **Beautiful Soup4のインストール**:\n",
        "  - `pip install bs4`でインストール。\n",
        "\n",
        "- **文章のスクレイピング**:\n",
        "  - `BeautifulSoup`と`urllib`を使用してWebページからテキストを取得。\n",
        "  - 必要な部分だけを抽出し、HTMLタグなどを除去。\n",
        "\n",
        "- **ストップワードリストの取得とクリーニング**:\n",
        "  - Webからストップワードリストを取得。\n",
        "  - リストを整理し、不要な文字を除去。\n",
        "\n",
        "#### ストップワードの除去\n",
        "- **除去プロセス**:\n",
        "  - 形態素解析で文章を品詞ごとに分割。\n",
        "  - ストップワードリストを用いて不要な単語を除去。\n",
        "\n",
        "- **具体的なコード例**:\n",
        "  - 分割された単語リストからストップワードを除去。\n",
        "\n"
      ],
      "metadata": {
        "id": "HVAlH7zaQmK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 正規表現モジュール`re`の使い方（コード例）\n",
        "1. **文字列の検索**:\n",
        "   - `re.match`の例:"
      ],
      "metadata": {
        "id": "1-IlKURCRFp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "email = 'abc@xxx.com'\n",
        "match_string = re.match(r'[a-z]+@[a-z]+\\.[a-z]+', email)\n",
        "print(match_string)\n",
        "#     - このコードは、与えられたメールアドレスが正規表現パターンに合致するかをチェックします。\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYP2YYWoRahS",
        "outputId": "c3b6eed0-7604-44d8-bab3-c8b165e6c64c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - `re.findall`の例:"
      ],
      "metadata": {
        "id": "YWv1KtGmRiDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emails = 'abc@xxx.com def@yyy.com test kimura@gmail.com'\n",
        "match_list = re.findall(r'[a-z]+@[a-z]+\\.[a-z]+', emails)\n",
        "print(match_list)\n",
        "#このコードは、文字列内のすべてのメールアドレスを見つけてリストで返します。\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG9rv4drRjI0",
        "outputId": "9c2ccded-2ce8-452d-ddfd-f02ba24cdeb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abc@xxx.com', 'def@yyy.com', 'kimura@gmail.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **文字列の置換**:\n",
        "   - `re.sub`の例:"
      ],
      "metadata": {
        "id": "QhVUwlx5Rqql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "email = 'abc@xxx.com'\n",
        "replace_string = re.sub(r'[a-z]+@', '[ABC]@', email)\n",
        "print(replace_string)\n",
        "# このコードは、メールアドレスのユーザー名部分を「ABC@」に置き換えます。\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcf-vG92RrWY",
        "outputId": "3d374660-887e-4526-c251-3f9d938670f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ABC]@xxx.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **テキストクリーニング**:\n",
        "   - 不要な記号の除去:"
      ],
      "metadata": {
        "id": "20vUSZHxR1mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '【重要です!】明日、必ず連絡を下さい'\n",
        "result = re.sub(r'[【】!]', ' ', text)\n",
        "print(result)\n",
        "#    - このコードは、特定の記号を空白に置き換えています。\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSKPzxr-R3MO",
        "outputId": "8544b5ea-20c0-4763-d7cb-25e0ec17035a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 重要です  明日、必ず連絡を下さい\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### スクレイピングと`Beautiful Soup4`の使用\n"
      ],
      "metadata": {
        "id": "4PhsliIDR-BI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Beautiful Soup4のインストール**:"
      ],
      "metadata": {
        "id": "1N2ukIFBR_lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knf_9r_FSB6h",
        "outputId": "7ee4ef04-d7be-486f-81a8-b630548cd3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=c4874dc6b04d8289236430908d7fb234ee89df10b3c54f8b04f10057349e122b\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built bs4\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **文章のスクレイピング**:\n",
        "   - `BeautifulSoup`と`urllib`を使用した例:"
      ],
      "metadata": {
        "id": "eWBONpY8SGhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib import request\n",
        "\n",
        "url = 'https://www.aozora.gr.jp/cards/000879/files/128_15261.html'\n",
        "response = request.urlopen(url)\n",
        "soup = BeautifulSoup(response)\n",
        "response.close()\n",
        "\n",
        "print(soup)\n",
        "#     - このコードは、青空文庫から「羅生門」の文章を取得します。\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEfC8VyESJma",
        "outputId": "0edd62e1-8232-4e91-8bce-67c623fbb310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"Shift_JIS\"?><!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\">\n",
            "<html xml:lang=\"ja\" xmlns=\"http://www.w3.org/1999/xhtml\">\n",
            "<head>\n",
            "<meta content=\"text/html;charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
            "<meta content=\"text/css\" http-equiv=\"content-style-type\"/>\n",
            "<link href=\"../../aozora.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
            "<title>芥川龍之介 羅生門</title>\n",
            "<script src=\"../../jquery-1.4.2.min.js\" type=\"text/javascript\"></script>\n",
            "<link href=\"http://purl.org/dc/elements/1.1/\" rel=\"Schema.DC\"/>\n",
            "<meta content=\"羅生門\" name=\"DC.Title\"/>\n",
            "<meta content=\"芥川龍之介\" name=\"DC.Creator\"/>\n",
            "<meta content=\"青空文庫\" name=\"DC.Publisher\"/>\n",
            "</head>\n",
            "<body>\n",
            "<div class=\"metadata\">\n",
            "<h1 class=\"title\">羅生門</h1>\n",
            "<h2 class=\"author\">芥川龍之介</h2>\n",
            "<br/>\n",
            "<br/>\n",
            "</div>\n",
            "<div id=\"contents\" style=\"display:none\"></div><div class=\"main_text\"><br/>\r\n",
            "　<ruby><rb>或日</rb><rp>（</rp><rt>あるひ</rt><rp>）</rp></ruby>の暮方の事である。一人の下人が、<ruby><rb>羅生門</rb><rp>（</rp><rt>らしやうもん</rt><rp>）</rp></ruby>の下で雨やみを待つてゐた。<br/>\r\n",
            "　廣い門の下には、この男の<ruby><rb>外</rb><rp>（</rp><rt>ほか</rt><rp>）</rp></ruby>に誰もゐない。唯、所々<ruby><rb>丹塗</rb><rp>（</rp><rt>にぬり</rt><rp>）</rp></ruby>の剥げた、大きな<ruby><rb>圓柱</rb><rp>（</rp><rt>まるばしら</rt><rp>）</rp></ruby>に、<ruby><rb>蟋蟀</rb><rp>（</rp><rt>きり／″＼す</rt><rp>）</rp></ruby>が一匹とまつてゐる。<ruby><rb>羅生門</rb><rp>（</rp><rt>らしやうもん</rt><rp>）</rp></ruby>が、<ruby><rb>朱雀大路</rb><rp>（</rp><rt>すじやくおおぢ</rt><rp>）</rp></ruby>にある<ruby><rb>以上</rb><rp>（</rp><rt>いじやう</rt><rp>）</rp></ruby>は、この男の外にも、<ruby><rb>雨</rb><rp>（</rp><rt>あめ</rt><rp>）</rp></ruby>やみをする<ruby><rb>市女笠</rb><rp>（</rp><rt>いちめがさ</rt><rp>）</rp></ruby>や揉烏帽子が、もう二三<ruby><rb>人</rb><rp>（</rp><rt>にん</rt><rp>）</rp></ruby>はありさうなものである。それが、この<ruby><rb>男</rb><rp>（</rp><rt>をとこ</rt><rp>）</rp></ruby>の<ruby><rb>外</rb><rp>（</rp><rt>ほか</rt><rp>）</rp></ruby>には<ruby><rb>誰</rb><rp>（</rp><rt>たれ</rt><rp>）</rp></ruby>もゐない。<br/>\r\n",
            "　<ruby><rb>何故</rb><rp>（</rp><rt>なぜ</rt><rp>）</rp></ruby>かと云ふと、この二三年、京都には、<ruby><rb>地震</rb><rp>（</rp><rt>ぢしん</rt><rp>）</rp></ruby>とか辻風とか火事とか饑饉とか云ふ<ruby><rb>災</rb><rp>（</rp><rt>わざはひ</rt><rp>）</rp></ruby>がつゞいて起つた。そこで<ruby><rb>洛中</rb><rp>（</rp><rt>らくちう</rt><rp>）</rp></ruby>のさびれ<ruby><rb>方</rb><rp>（</rp><rt>かた</rt><rp>）</rp></ruby>は一通りでない。舊記によると、佛像や佛具を<ruby><rb>打砕</rb><rp>（</rp><rt>うちくだ</rt><rp>）</rp></ruby>いて、その<ruby><rb>丹</rb><rp>（</rp><rt>に</rt><rp>）</rp></ruby>がついたり、金銀の<ruby><rb>箔</rb><rp>（</rp><rt>はく</rt><rp>）</rp></ruby>がついたりした木を、路ばたにつみ重ねて、<ruby><rb>薪</rb><rp>（</rp><rt>たきぎ</rt><rp>）</rp></ruby>の<ruby><rb>料</rb><rp>（</rp><rt>しろ</rt><rp>）</rp></ruby>に賣つてゐたと云ふ事である。<ruby><rb>洛中</rb><rp>（</rp><rt>らくちう</rt><rp>）</rp></ruby>がその始末であるから、羅生門の<ruby><rb>修理</rb><rp>（</rp><rt>しゆり</rt><rp>）</rp></ruby>などは、元より誰も捨てゝ<ruby><rb>顧</rb><rp>（</rp><rt>かへりみ</rt><rp>）</rp></ruby>る者がなかつた。するとその<ruby><rb>荒</rb><rp>（</rp><rt>あ</rt><rp>）</rp></ruby>れ<ruby><rb>果</rb><rp>（</rp><rt>は</rt><rp>）</rp></ruby>てたのをよい事にして、<ruby><rb>狐狸</rb><rp>（</rp><rt>こり</rt><rp>）</rp></ruby>が棲む。<ruby><rb>盗人</rb><rp>（</rp><rt>ぬすびと</rt><rp>）</rp></ruby>が棲む。とうとうしまひには、<ruby><rb>引取</rb><rp>（</rp><rt>ひきと</rt><rp>）</rp></ruby>り手のない死人を、この門へ持つて來て、棄てゝ行くと云ふ<ruby><rb>習慣</rb><rp>（</rp><rt>しふくわん</rt><rp>）</rp></ruby>さへ出來た。そこで、日の目が見えなくなると、誰でも<ruby><rb>氣味</rb><rp>（</rp><rt>きみ</rt><rp>）</rp></ruby>を惡るがつて、この門の<ruby><rb>近所</rb><rp>（</rp><rt>きんじよ</rt><rp>）</rp></ruby>へは<ruby><rb>足</rb><rp>（</rp><rt>あし</rt><rp>）</rp></ruby>ぶみをしない事になつてしまつたのである。<br/>\r\n",
            "　その代り又<ruby><rb>鴉</rb><rp>（</rp><rt>からす</rt><rp>）</rp></ruby>が<ruby><rb>何處</rb><rp>（</rp><rt>どこ</rt><rp>）</rp></ruby>からか、たくさん集つて來た。<ruby><rb>晝間</rb><rp>（</rp><rt>ひるま</rt><rp>）</rp></ruby><ruby><rb>見</rb><rp>（</rp><rt>み</rt><rp>）</rp></ruby>ると、その鴉が<ruby><rb>何羽</rb><rp>（</rp><rt>なんば</rt><rp>）</rp></ruby>となく輪を描いて高い<ruby><rb>鴟尾</rb><rp>（</rp><rt>しび</rt><rp>）</rp></ruby>のまはりを<ruby><rb>啼</rb><rp>（</rp><rt>な</rt><rp>）</rp></ruby>きながら、飛びまはつてゐる。殊に門の上の空が、<ruby><rb>夕燒</rb><rp>（</rp><rt>ゆふや</rt><rp>）</rp></ruby>けであかくなる<ruby><rb>時</rb><rp>（</rp><rt>とき</rt><rp>）</rp></ruby>には、それが<ruby><rb>胡麻</rb><rp>（</rp><rt>ごま</rt><rp>）</rp></ruby>をまいたやうにはつきり見えた。<ruby><rb>鴉</rb><rp>（</rp><rt>からす</rt><rp>）</rp></ruby>は、勿論、門の上にある<ruby><rb>死人</rb><rp>（</rp><rt>しにん</rt><rp>）</rp></ruby>の肉を、啄みに來るのである。――尤も今日は、<ruby><rb>刻限</rb><rp>（</rp><rt>こくげん</rt><rp>）</rp></ruby>が<ruby><rb>遲</rb><rp>（</rp><rt>おそ</rt><rp>）</rp></ruby>いせいか、一羽も見えない。唯、<ruby><rb>所々</rb><rp>（</rp><rt>ところどころ</rt><rp>）</rp></ruby>、崩れかゝつた、さうしてその<ruby><rb>崩</rb><rp>（</rp><rt>くづ</rt><rp>）</rp></ruby>れ目に長い草のはへた<ruby><rb>石段</rb><rp>（</rp><rt>いしだん</rt><rp>）</rp></ruby>の上に、<ruby><rb>鴉</rb><rp>（</rp><rt>からす</rt><rp>）</rp></ruby>の<ruby><rb>糞</rb><rp>（</rp><rt>くそ</rt><rp>）</rp></ruby>が、點々と白くこびりついてゐるのが見える。<ruby><rb>下人</rb><rp>（</rp><rt>げにん</rt><rp>）</rp></ruby>は七段ある石段の一番上の<ruby><rb>段</rb><rp>（</rp><rt>だん</rt><rp>）</rp></ruby>に<ruby><rb>洗</rb><rp>（</rp><rt>あら</rt><rp>）</rp></ruby>ひざらした<ruby><rb>紺</rb><rp>（</rp><rt>こん</rt><rp>）</rp></ruby>の<ruby><rb>襖</rb><rp>（</rp><rt>あを</rt><rp>）</rp></ruby>の尻を据ゑて、右の頬に出來た、大きな<ruby><rb>面皰</rb><rp>（</rp><rt>にきび</rt><rp>）</rp></ruby>を氣にしながら、ぼんやり、<ruby><rb>雨</rb><rp>（</rp><rt>あめ</rt><rp>）</rp></ruby>のふるのを<ruby><rb>眺</rb><rp>（</rp><rt>なが</rt><rp>）</rp></ruby>めてゐるのである。<br/>\r\n",
            "　<ruby><rb>作者</rb><rp>（</rp><rt>さくしや</rt><rp>）</rp></ruby>はさつき、「下人が雨やみを待つてゐた」と書いた。しかし、<ruby><rb>下人</rb><rp>（</rp><rt>げにん</rt><rp>）</rp></ruby>は、雨がやんでも<ruby><rb>格別</rb><rp>（</rp><rt>かくべつ</rt><rp>）</rp></ruby>どうしようと云ふ當てはない。ふだんなら、<ruby><rb>勿論</rb><rp>（</rp><rt>もちろん</rt><rp>）</rp></ruby>、主人の家へ歸る可き筈である。<ruby><rb>所</rb><rp>（</rp><rt>ところ</rt><rp>）</rp></ruby>がその主人からは、四五日前に<ruby><rb>暇</rb><rp>（</rp><rt>ひま</rt><rp>）</rp></ruby>を<ruby><rb>出</rb><rp>（</rp><rt>だ</rt><rp>）</rp></ruby>された。前にも書いたやうに、<ruby><rb>當時</rb><rp>（</rp><rt>たうじ</rt><rp>）</rp></ruby><ruby><rb>京都</rb><rp>（</rp><rt>きやうと</rt><rp>）</rp></ruby>の町は一通りならず<ruby><rb>衰微</rb><rp>（</rp><rt>すゐび</rt><rp>）</rp></ruby>してゐた。今この下人が、<ruby><rb>永年</rb><rp>（</rp><rt>ながねん</rt><rp>）</rp></ruby>、使はれてゐた主人から、<ruby><rb>暇</rb><rp>（</rp><rt>ひま</rt><rp>）</rp></ruby>を出されたのも、この衰微の小さな餘波に外ならない。だから「下人が<ruby><rb>雨</rb><rp>（</rp><rt>あめ</rt><rp>）</rp></ruby>やみを待つてゐた」と<ruby><rb>云</rb><rp>（</rp><rt>い</rt><rp>）</rp></ruby>ふよりも、「雨にふりこめられた下人が、<ruby><rb>行</rb><rp>（</rp><rt>ゆ</rt><rp>）</rp></ruby>き<ruby><rb>所</rb><rp>（</rp><rt>どころ</rt><rp>）</rp></ruby>がなくて、途方にくれてゐた」と云ふ方が、<ruby><rb>適當</rb><rp>（</rp><rt>てきたう</rt><rp>）</rp></ruby>である。その上、今日の<ruby><rb>空模樣</rb><rp>（</rp><rt>そらもやう</rt><rp>）</rp></ruby>も少からずこの<ruby><rb>平安朝</rb><rp>（</rp><rt>へいあんてう</rt><rp>）</rp></ruby>の下人の Sentimentalisme に<ruby><rb>影響</rb><rp>（</rp><rt>えいきやう</rt><rp>）</rp></ruby>した。<ruby><rb>申</rb><rp>（</rp><rt>さる</rt><rp>）</rp></ruby>の刻下りからふり出した雨は、未に<ruby><rb>上</rb><rp>（</rp><rt>あが</rt><rp>）</rp></ruby>るけしきがない。そこで、下人は、何を措いても<ruby><rb>差當</rb><rp>（</rp><rt>さしあた</rt><rp>）</rp></ruby>り明日の<ruby><rb>暮</rb><rp>（</rp><rt>くら</rt><rp>）</rp></ruby>しをどうにかしようとして――云はゞどうにもならない<ruby><rb>事</rb><rp>（</rp><rt>こと</rt><rp>）</rp></ruby>を、どうにかしようとして、とりとめもない<ruby><rb>考</rb><rp>（</rp><rt>かんが</rt><rp>）</rp></ruby>へをたどりながら、さつきから<ruby><rb>朱雀大路</rb><rp>（</rp><rt>すじやくおはぢ</rt><rp>）</rp></ruby>にふる雨の音を、聞くともなく聞いてゐた。<br/>\r\n",
            "　雨は、<ruby><rb>羅生門</rb><rp>（</rp><rt>らしやうもん</rt><rp>）</rp></ruby>をつゝんで、<ruby><rb>遠</rb><rp>（</rp><rt>とほ</rt><rp>）</rp></ruby>くから、ざあつと云ふ音をあつめて來る。夕闇は次第に空を低くして、<ruby><rb>見上</rb><rp>（</rp><rt>みあ</rt><rp>）</rp></ruby>げると、門の屋根が、斜につき出した<ruby><rb>甍</rb><rp>（</rp><rt>いらか</rt><rp>）</rp></ruby><span class=\"notes\">［＃「甍の」は底本では「薨の」］</span><ruby><rb>先</rb><rp>（</rp><rt>さき</rt><rp>）</rp></ruby>に、重たくうす<ruby><rb>暗</rb><rp>（</rp><rt>くら</rt><rp>）</rp></ruby>い<ruby><rb>雲</rb><rp>（</rp><rt>くも</rt><rp>）</rp></ruby>を支へてゐる。<br/>\r\n",
            "　どうにもならない事を、どうにかする爲には、<ruby><rb>手段</rb><rp>（</rp><rt>しゆだん</rt><rp>）</rp></ruby>を選んでゐる<ruby><rb>遑</rb><rp>（</rp><rt>いとま</rt><rp>）</rp></ruby>はない。選んでゐれば、<ruby><rb>築土</rb><rp>（</rp><rt>ついぢ</rt><rp>）</rp></ruby>の下か、道ばたの土の上で、<ruby><rb>饑死</rb><rp>（</rp><rt>うゑじに</rt><rp>）</rp></ruby>をするばかりである。さうして、この門の上へ持つて來て、<ruby><rb>犬</rb><rp>（</rp><rt>いぬ</rt><rp>）</rp></ruby>のやうに<ruby><rb>棄</rb><rp>（</rp><rt>す</rt><rp>）</rp></ruby>てられてしまふばかりである。<ruby><rb>選</rb><rp>（</rp><rt>えら</rt><rp>）</rp></ruby>ばないとすれば――下人の考へは、<ruby><rb>何度</rb><rp>（</rp><rt>なんど</rt><rp>）</rp></ruby>も同じ道を低徊した<ruby><rb>揚句</rb><rp>（</rp><rt>あげく</rt><rp>）</rp></ruby>に、やつとこの局所へ<ruby><rb>逢着</rb><rp>（</rp><rt>はうちやく</rt><rp>）</rp></ruby>した。しかしこの「すれば」は、<ruby><rb>何時</rb><rp>（</rp><rt>いつ</rt><rp>）</rp></ruby>までたつても、結局「すれば」であつた。下人は、<ruby><rb>手段</rb><rp>（</rp><rt>しゆだん</rt><rp>）</rp></ruby>を選ばないといふ事を<ruby><rb>肯定</rb><rp>（</rp><rt>こうてい</rt><rp>）</rp></ruby>しながらも、この「すれば」のかたをつける爲に、<ruby><rb>當然</rb><rp>（</rp><rt>たうぜん</rt><rp>）</rp></ruby>、その後に來る可き「<ruby><rb>盗人</rb><rp>（</rp><rt>ぬすびと</rt><rp>）</rp></ruby>になるより外に<ruby><rb>仕方</rb><rp>（</rp><rt>しかた</rt><rp>）</rp></ruby>がない」と云ふ事を、<ruby><rb>積極的</rb><rp>（</rp><rt>せきゝよくてき</rt><rp>）</rp></ruby>に肯定する丈の、勇氣が出ずにゐたのである。<br/>\r\n",
            "　下人は、大きな<ruby><rb>嚏</rb><rp>（</rp><rt>くさめ</rt><rp>）</rp></ruby>をして、それから、大儀さうに立上つた。<ruby><rb>夕冷</rb><rp>（</rp><rt>ゆふひ</rt><rp>）</rp></ruby>えのする京都は、もう<ruby><rb>火桶</rb><rp>（</rp><rt>ひをけ</rt><rp>）</rp></ruby>が欲しい程の寒さである。風は門の<ruby><rb>柱</rb><rp>（</rp><rt>はしら</rt><rp>）</rp></ruby>と柱との間を、夕闇と共に遠慮なく、吹きぬける。<ruby><rb>丹塗</rb><rp>（</rp><rt>にぬり</rt><rp>）</rp></ruby>の柱にとまつてゐた<ruby><rb>蟋蟀</rb><rp>（</rp><rt>きり／″＼す</rt><rp>）</rp></ruby>も、もうどこかへ行つてしまつた。<br/>\r\n",
            "　下人は、頸をちゞめながら、山吹の<ruby><rb>汗衫</rb><rp>（</rp><rt>かざみ</rt><rp>）</rp></ruby>に重ねた、紺の襖の肩を<ruby><rb>高</rb><rp>（</rp><rt>たか</rt><rp>）</rp></ruby>くして門のまはりを見まはした。<ruby><rb>雨風</rb><rp>（</rp><rt>あめかぜ</rt><rp>）</rp></ruby>の患のない、人目にかゝる惧のない、一<ruby><rb>晩</rb><rp>（</rp><rt>ばん</rt><rp>）</rp></ruby><ruby><rb>樂</rb><rp>（</rp><rt>らく</rt><rp>）</rp></ruby>にねられさうな所があれば、そこでともかくも、<ruby><rb>夜</rb><rp>（</rp><rt>よ</rt><rp>）</rp></ruby>を<ruby><rb>明</rb><rp>（</rp><rt>あ</rt><rp>）</rp></ruby><span class=\"notes\">［＃ルビの「あ」は底本では「あか」］</span>かさうと思つたからである。すると、幸門の上の<ruby><rb>樓</rb><rp>（</rp><rt>ろう</rt><rp>）</rp></ruby>へ上る、幅の廣い、之も丹を塗つた<ruby><rb>梯子</rb><rp>（</rp><rt>はしご</rt><rp>）</rp></ruby>が眼についた。<ruby><rb>上</rb><rp>（</rp><rt>うへ</rt><rp>）</rp></ruby>なら、人がゐたにしても、どうせ<ruby><rb>死人</rb><rp>（</rp><rt>しにん</rt><rp>）</rp></ruby>ばかりである。下人は、そこで腰にさげた<ruby><rb>聖柄</rb><rp>（</rp><rt>ひぢりづか</rt><rp>）</rp></ruby>の太刀が鞘走らないやうに氣をつけながら、<ruby><rb>藁草履</rb><rp>（</rp><rt>わらざうり</rt><rp>）</rp></ruby>をはいた足を、その梯子の一<ruby><rb>番下</rb><rp>（</rp><rt>ばんした</rt><rp>）</rp></ruby>の段へふみかけた。<br/>\r\n",
            "　それから、<ruby><rb>何分</rb><rp>（</rp><rt>なんぷん</rt><rp>）</rp></ruby>かの後である。羅生門の樓の上へ出る、<ruby><rb>幅</rb><rp>（</rp><rt>はゞ</rt><rp>）</rp></ruby>の廣い梯子の中段に、一人の男が、<ruby><rb>猫</rb><rp>（</rp><rt>ねこ</rt><rp>）</rp></ruby>のやうに身をちゞめて、<ruby><rb>息</rb><rp>（</rp><rt>いき</rt><rp>）</rp></ruby>を殺しながら、上の<ruby><rb>容子</rb><rp>（</rp><rt>ようす</rt><rp>）</rp></ruby>を窺つてゐた。樓の上からさす<ruby><rb>火</rb><rp>（</rp><rt>ひ</rt><rp>）</rp></ruby>の<ruby><rb>光</rb><rp>（</rp><rt>ひかり</rt><rp>）</rp></ruby>が、かすかに、その男の右の<ruby><rb>頬</rb><rp>（</rp><rt>ほゝ</rt><rp>）</rp></ruby>をぬらしてゐる。短い<ruby><rb>鬚</rb><rp>（</rp><rt>ひげ</rt><rp>）</rp></ruby>の中に、赤く膿を持つた<ruby><rb>面皰</rb><rp>（</rp><rt>にきび</rt><rp>）</rp></ruby>のある頬である。下人は、始めから、この上にゐる者は、<ruby><rb>死人</rb><rp>（</rp><rt>しにん</rt><rp>）</rp></ruby>ばかりだと高を括つてゐた。それが、<ruby><rb>梯子</rb><rp>（</rp><rt>はしご</rt><rp>）</rp></ruby>を二三段上つて見ると、上では誰か<ruby><rb>火</rb><rp>（</rp><rt>ひ</rt><rp>）</rp></ruby>をとぼして、しかもその火を<ruby><rb>其處此處</rb><rp>（</rp><rt>そこゝこ</rt><rp>）</rp></ruby>と<ruby><rb>動</rb><rp>（</rp><rt>うご</rt><rp>）</rp></ruby>かしてゐるらしい。これは、その濁つた、黄いろい光が、<ruby><rb>隅々</rb><rp>（</rp><rt>すみ／″＼</rt><rp>）</rp></ruby>に蜘蛛の巣をかけた天井裏に、ゆれながら<ruby><rb>映</rb><rp>（</rp><rt>うつ</rt><rp>）</rp></ruby>つたので、すぐにそれと知れたのである。この雨の夜に、この羅生門の上で、火をともしてゐるからは、どうせ唯の者ではない。<br/>\r\n",
            "　下人は、<ruby><rb>守宮</rb><rp>（</rp><rt>やもり</rt><rp>）</rp></ruby>のやうに足音をぬすんで、やつと<ruby><rb>急</rb><rp>（</rp><rt>きふ</rt><rp>）</rp></ruby>な梯子を、一番上の段まで這ふやうにして上りつめた。さうして<ruby><rb>體</rb><rp>（</rp><rt>からだ</rt><rp>）</rp></ruby>を出來る丈、平にしながら、<ruby><rb>頸</rb><rp>（</rp><rt>くび</rt><rp>）</rp></ruby>を出來る丈、前へ出して、<ruby><rb>恐</rb><rp>（</rp><rt>おそ</rt><rp>）</rp></ruby>る恐る、樓の内を<ruby><rb>覗</rb><rp>（</rp><rt>のぞ</rt><rp>）</rp></ruby>いて見た。<br/>\r\n",
            "　見ると、樓の内には、<ruby><rb>噂</rb><rp>（</rp><rt>うはさ</rt><rp>）</rp></ruby>に聞いた通り、幾つかの<ruby><rb>屍骸</rb><rp>（</rp><rt>しがい</rt><rp>）</rp></ruby>が、<ruby><rb>無造作</rb><rp>（</rp><rt>むざうさ</rt><rp>）</rp></ruby>に棄てゝあるが、火の光の及ぶ<ruby><rb>範圍</rb><rp>（</rp><rt>はんゐ</rt><rp>）</rp></ruby>が、思つたより狹いので、<ruby><rb>數</rb><rp>（</rp><rt>かず</rt><rp>）</rp></ruby>は幾つともわからない。唯、おぼろげながら、知れるのは、その中に<ruby><rb>裸</rb><rp>（</rp><rt>はだか</rt><rp>）</rp></ruby>の屍骸と、<ruby><rb>着物</rb><rp>（</rp><rt>きもの</rt><rp>）</rp></ruby>を着た屍骸とがあると云ふ事である。<ruby><rb>勿論</rb><rp>（</rp><rt>もちろん</rt><rp>）</rp></ruby>、中には女も男もまじつてゐるらしい。さうして、その屍骸は皆、それが、甞、生きてゐた人間だと云ふ<ruby><rb>事實</rb><rp>（</rp><rt>じゞつ</rt><rp>）</rp></ruby>さへ疑はれる程、土を捏ねて造つた<ruby><rb>人形</rb><rp>（</rp><rt>にんぎやう</rt><rp>）</rp></ruby>のやうに、口を<ruby><rb>開</rb><rp>（</rp><rt>あ</rt><rp>）</rp></ruby>いたり手を延ばしたりしてごろごろ<ruby><rb>床</rb><rp>（</rp><rt>ゆか</rt><rp>）</rp></ruby>の上にころがつてゐた。しかも、肩とか<ruby><rb>胸</rb><rp>（</rp><rt>むね</rt><rp>）</rp></ruby>とかの高くなつてゐる<ruby><rb>部分</rb><rp>（</rp><rt>ぶゞん</rt><rp>）</rp></ruby>に、ぼんやりした火の光をうけて、低くなつてゐる部分の影を一<ruby><rb>層</rb><rp>（</rp><rt>そう</rt><rp>）</rp></ruby><ruby><rb>暗</rb><rp>（</rp><rt>くら</rt><rp>）</rp></ruby>くしながら、永久に<ruby><rb>唖</rb><rp>（</rp><rt>おし</rt><rp>）</rp></ruby>の如く<ruby><rb>默</rb><rp>（</rp><rt>だま</rt><rp>）</rp></ruby>つていた。<br/>\r\n",
            "　下人は、それらの屍骸の<ruby><rb>腐爛</rb><rp>（</rp><rt>ふらん</rt><rp>）</rp></ruby>した臭氣に思はず、<ruby><rb>鼻</rb><rp>（</rp><rt>はな</rt><rp>）</rp></ruby>を掩つた。しかし、その手は、次の<ruby><rb>瞬間</rb><rp>（</rp><rt>しゆんかん</rt><rp>）</rp></ruby>には、もう鼻を掩ふ事を忘れてゐた。或る強い<ruby><rb>感情</rb><rp>（</rp><rt>かんじやう</rt><rp>）</rp></ruby>が、殆悉この男の嗅覺を奪つてしまつたからである。<br/>\r\n",
            "　下人の眼は、その時、はじめて、<ruby><rb>其</rb><rp>（</rp><rt>その</rt><rp>）</rp></ruby><ruby><rb>屍骸</rb><rp>（</rp><rt>しがい</rt><rp>）</rp></ruby>の中に蹲つている人間を見た。<ruby><rb>檜肌色</rb><rp>（</rp><rt>ひはだいろ</rt><rp>）</rp></ruby>の着物を著た、背の低い、痩せた、<ruby><rb>白髮頭</rb><rp>（</rp><rt>しらがあたま</rt><rp>）</rp></ruby>の、猿のやうな老婆である。その老婆は、右の手に火をともした<ruby><rb>松</rb><rp>（</rp><rt>まつ</rt><rp>）</rp></ruby>の木片を持つて、その<ruby><rb>屍骸</rb><rp>（</rp><rt>しがい</rt><rp>）</rp></ruby>の一つの顏を覗きこむやうに<ruby><rb>眺</rb><rp>（</rp><rt>なが</rt><rp>）</rp></ruby>めてゐた。髮の毛の長い所を見ると、<ruby><rb>多分</rb><rp>（</rp><rt>たぶん</rt><rp>）</rp></ruby><ruby><rb>女</rb><rp>（</rp><rt>をんな</rt><rp>）</rp></ruby>の屍骸であらう。<br/>\r\n",
            "　下人は、六分の<ruby><rb>恐怖</rb><rp>（</rp><rt>きやうふ</rt><rp>）</rp></ruby>と四分の好奇心とに動かされて、暫時は<ruby><rb>呼吸</rb><rp>（</rp><rt>いき</rt><rp>）</rp></ruby>をするのさへ忘れてゐた。舊記の<ruby><rb>記者</rb><rp>（</rp><rt>きしや</rt><rp>）</rp></ruby>の語を借りれば、「<ruby><rb>頭身</rb><rp>（</rp><rt>とうしん</rt><rp>）</rp></ruby>の毛も太る」やうに感じたのである。すると、<ruby><rb>老婆</rb><rp>（</rp><rt>らうば</rt><rp>）</rp></ruby>は、松の木片を、床板の間に<ruby><rb>挿</rb><rp>（</rp><rt>さ</rt><rp>）</rp></ruby>して、それから、今まで眺めてゐた屍骸の首に<ruby><rb>兩手</rb><rp>（</rp><rt>りやうて</rt><rp>）</rp></ruby>をかけると、丁度、猿の親が猿の子の<ruby><rb>虱</rb><rp>（</rp><rt>しらみ</rt><rp>）</rp></ruby>をとるやうに、その長い<ruby><rb>髮</rb><rp>（</rp><rt>かみ</rt><rp>）</rp></ruby>の<ruby><rb>毛</rb><rp>（</rp><rt>け</rt><rp>）</rp></ruby>を一本づゝ拔きはじめた。髮は手に<ruby><rb>從</rb><rp>（</rp><rt>したが</rt><rp>）</rp></ruby>つて拔けるらしい。<br/>\r\n",
            "　その髮の毛が、一本ずゝ<ruby><rb>拔</rb><rp>（</rp><rt>ぬ</rt><rp>）</rp></ruby>けるのに從つて下人の<ruby><rb>心</rb><rp>（</rp><rt>こゝろ</rt><rp>）</rp></ruby>からは、恐怖が少しづつ消えて行つた。さうして、それと<ruby><rb>同時</rb><rp>（</rp><rt>どうじ</rt><rp>）</rp></ruby>に、この老婆に對するはげしい<ruby><rb>憎惡</rb><rp>（</rp><rt>ぞうを</rt><rp>）</rp></ruby>が、少しづゝ動いて來た。――いや、この<ruby><rb>老婆</rb><rp>（</rp><rt>らうば</rt><rp>）</rp></ruby>に對すると云つては、<ruby><rb>語弊</rb><rp>（</rp><rt>ごへい</rt><rp>）</rp></ruby>があるかも知れない。寧、あらゆる惡に對する<ruby><rb>反感</rb><rp>（</rp><rt>はんかん</rt><rp>）</rp></ruby>が、一分毎に強さを増して來たのである。この時、<ruby><rb>誰</rb><rp>（</rp><rt>たれ</rt><rp>）</rp></ruby>かがこの下人に、さつき<ruby><rb>門</rb><rp>（</rp><rt>もん</rt><rp>）</rp></ruby>の下でこの男が考へてゐた、<ruby><rb>饑死</rb><rp>（</rp><rt>うゑじに</rt><rp>）</rp></ruby>をするか盗人になるかと云ふ問題を、改めて<ruby><rb>持出</rb><rp>（</rp><rt>もちだ</rt><rp>）</rp></ruby>したら、恐らく下人は、何の<ruby><rb>未練</rb><rp>（</rp><rt>みれん</rt><rp>）</rp></ruby>もなく、饑死を選んだ事であらう。それほど、この<ruby><rb>男</rb><rp>（</rp><rt>をとこ</rt><rp>）</rp></ruby>の惡を憎む心は、老婆の<ruby><rb>床</rb><rp>（</rp><rt>ゆか</rt><rp>）</rp></ruby>に挿した松の木片のやうに、勢よく<ruby><rb>燃</rb><rp>（</rp><rt>も</rt><rp>）</rp></ruby>え<ruby><rb>上</rb><rp>（</rp><rt>あが</rt><rp>）</rp></ruby>り出してゐたのである。<br/>\r\n",
            "　下人には、勿論、何故老婆が<ruby><rb>死人</rb><rp>（</rp><rt>しにん</rt><rp>）</rp></ruby>の髮の毛を<ruby><rb>拔</rb><rp>（</rp><rt>ぬ</rt><rp>）</rp></ruby>くかわからなかつた。從つて、<ruby><rb>合理的</rb><rp>（</rp><rt>がふりてき</rt><rp>）</rp></ruby>には、それを善惡の何れに<ruby><rb>片</rb><rp>（</rp><rt>かた</rt><rp>）</rp></ruby>づけてよいか知らなかつた。しかし下人にとつては、この<ruby><rb>雨</rb><rp>（</rp><rt>あめ</rt><rp>）</rp></ruby>の<ruby><rb>夜</rb><rp>（</rp><rt>よ</rt><rp>）</rp></ruby>に、この羅生門の上で、死人の髮の<ruby><rb>毛</rb><rp>（</rp><rt>け</rt><rp>）</rp></ruby>を拔くと云ふ事が、それ丈で既に<ruby><rb>許</rb><rp>（</rp><rt>ゆる</rt><rp>）</rp></ruby>す可らざる惡であつた。勿論、<ruby><rb>下人</rb><rp>（</rp><rt>げにん</rt><rp>）</rp></ruby>は、さつき迄自分が、盗人になる氣でゐた事なぞは、とうに忘れてゐるのである。<br/>\r\n",
            "　そこで、下人は、<ruby><rb>兩足</rb><rp>（</rp><rt>りやうあし</rt><rp>）</rp></ruby>に力を入れて、いきなり、<ruby><rb>梯子</rb><rp>（</rp><rt>はしご</rt><rp>）</rp></ruby>から上へ飛び上つた。さうして<ruby><rb>聖柄</rb><rp>（</rp><rt>ひぢりづか</rt><rp>）</rp></ruby>の太刀に手をかけながら、<ruby><rb>大股</rb><rp>（</rp><rt>おおまた</rt><rp>）</rp></ruby>に老婆の前へ歩みよつた。老婆が驚いたのは、云ふ迄もない。<br/>\r\n",
            "　老婆は、一目下人を見ると、まるで<ruby><rb>弩</rb><rp>（</rp><rt>いしゆみ</rt><rp>）</rp></ruby>にでも弾かれたやうに、飛び上つた。<br/>\r\n",
            "「おのれ、どこへ行く。」<br/>\r\n",
            "　下人は、老婆が<ruby><rb>屍骸</rb><rp>（</rp><rt>しがい</rt><rp>）</rp></ruby>につまづきながら、<ruby><rb>慌</rb><rp>（</rp><rt>あは</rt><rp>）</rp></ruby>てふためいて逃げようとする行手を塞いで、こう<ruby><rb>罵</rb><rp>（</rp><rt>のゝし</rt><rp>）</rp></ruby>つた。老婆は、それでも下人をつきのけて<ruby><rb>行</rb><rp>（</rp><rt>ゆ</rt><rp>）</rp></ruby>かうとする。下人は又、それを行かすまいとして、<ruby><rb>押</rb><rp>（</rp><rt>お</rt><rp>）</rp></ruby>しもどす。二人は<ruby><rb>屍骸</rb><rp>（</rp><rt>しがい</rt><rp>）</rp></ruby>の中で、暫、<ruby><rb>無言</rb><rp>（</rp><rt>むごん</rt><rp>）</rp></ruby>のまゝ、つかみ合つた。しかし<ruby><rb>勝敗</rb><rp>（</rp><rt>しようはい</rt><rp>）</rp></ruby>は、はじめから、わかつている。下人はとうとう、老婆の<ruby><rb>腕</rb><rp>（</rp><rt>うで</rt><rp>）</rp></ruby>をつかんで、無理にそこへ<ruby><rb><img alt=\"※(「てへん＋丑」、第4水準2-12-93)\" class=\"gaiji\" src=\"../../../gaiji/2-12/2-12-93.png\"/></rb><rp>（</rp><rt>ね</rt><rp>）</rp></ruby>ぢ<ruby><rb>倒</rb><rp>（</rp><rt>たほ</rt><rp>）</rp></ruby>した。丁度、<ruby><rb>鷄</rb><rp>（</rp><rt>とり</rt><rp>）</rp></ruby>の脚のやうな、骨と皮ばかりの腕である。<br/>\r\n",
            "「何をしてゐた。さあ何をしてゐた。云へ。云はぬと、これだぞよ。」<br/>\r\n",
            "　下人は、<ruby><rb>老婆</rb><rp>（</rp><rt>らうば</rt><rp>）</rp></ruby>をつき放すと、いきなり、<ruby><rb>太刀</rb><rp>（</rp><rt>たち</rt><rp>）</rp></ruby>の<ruby><rb>鞘</rb><rp>（</rp><rt>さや</rt><rp>）</rp></ruby>を拂つて、白い<ruby><rb>鋼</rb><rp>（</rp><rt>はがね</rt><rp>）</rp></ruby>の色をその眼の前へつきつけた。けれども、老婆は默つてゐる。<ruby><rb>兩手</rb><rp>（</rp><rt>りやうて</rt><rp>）</rp></ruby>をわなわなふるはせて、肩で<ruby><rb>息</rb><rp>（</rp><rt>いき</rt><rp>）</rp></ruby>を切りながら、眼を、<ruby><rb>眼球</rb><rp>（</rp><rt>がんきう</rt><rp>）</rp></ruby>が<ruby><rb><img alt=\"※(「目＋匡」、第3水準1-88-81)\" class=\"gaiji\" src=\"../../../gaiji/1-88/1-88-81.png\"/></rb><rp>（</rp><rt>まぶた</rt><rp>）</rp></ruby>の外へ出さうになる程、見開いて、唖のやうに<ruby><rb>執拗</rb><rp>（</rp><rt>しうね</rt><rp>）</rp></ruby>く默つてゐる。これを見ると、下人は<ruby><rb>始</rb><rp>（</rp><rt>はじ</rt><rp>）</rp></ruby>めて明白にこの老婆の生死が、全然、自分の<ruby><rb>意志</rb><rp>（</rp><rt>いし</rt><rp>）</rp></ruby>に支配されてゐると云ふ事を<ruby><rb>意識</rb><rp>（</rp><rt>いしき</rt><rp>）</rp></ruby>した。さうして、この意識は、<ruby><rb>今</rb><rp>（</rp><rt>いま</rt><rp>）</rp></ruby>まではげしく燃えてゐた憎惡の心を<ruby><rb>何時</rb><rp>（</rp><rt>いつ</rt><rp>）</rp></ruby>の間にか<ruby><rb>冷</rb><rp>（</rp><rt>さ</rt><rp>）</rp></ruby>ましてしまつた。<ruby><rb>後</rb><rp>（</rp><rt>あと</rt><rp>）</rp></ruby>に殘つたのは、唯、<ruby><rb>或</rb><rp>（</rp><rt>ある</rt><rp>）</rp></ruby><ruby><rb>仕事</rb><rp>（</rp><rt>しごと</rt><rp>）</rp></ruby>をして、それが<ruby><rb>圓滿</rb><rp>（</rp><rt>ゑんまん</rt><rp>）</rp></ruby>に成就した時の、安らかな<ruby><rb>得意</rb><rp>（</rp><rt>とくい</rt><rp>）</rp></ruby>と滿足とがあるばかりである。そこで、下人は、<ruby><rb>老婆</rb><rp>（</rp><rt>らうば</rt><rp>）</rp></ruby>を見下しながら、少し聲を<ruby><rb>柔</rb><rp>（</rp><rt>やはら</rt><rp>）</rp></ruby>げてかう云つた。<br/>\r\n",
            "「己は<ruby><rb>檢非違使</rb><rp>（</rp><rt>けびゐし</rt><rp>）</rp></ruby>の廳の役人などではない。今し方この<ruby><rb>門</rb><rp>（</rp><rt>もん</rt><rp>）</rp></ruby>の下を<ruby><rb>通</rb><rp>（</rp><rt>とほ</rt><rp>）</rp></ruby>りかゝつた旅の者だ。だからお前に<ruby><rb>繩</rb><rp>（</rp><rt>なわ</rt><rp>）</rp></ruby>をかけて、どうしようと云ふやうな事はない。<ruby><rb>唯</rb><rp>（</rp><rt>たゞ</rt><rp>）</rp></ruby>、今時分、この門の上で、<ruby><rb>何</rb><rp>（</rp><rt>なに</rt><rp>）</rp></ruby>をして居たのだか、それを己に<ruby><rb>話</rb><rp>（</rp><rt>はなし</rt><rp>）</rp></ruby>しさへすればいいのだ。」<br/>\r\n",
            "　すると、老婆は、<ruby><rb>見開</rb><rp>（</rp><rt>みひら</rt><rp>）</rp></ruby>いてゐた眼を、一<ruby><rb>層大</rb><rp>（</rp><rt>そうおほ</rt><rp>）</rp></ruby>きくして、ぢつとその下人の<ruby><rb>顏</rb><rp>（</rp><rt>かほ</rt><rp>）</rp></ruby>を見守つた。<img alt=\"※(「目＋匡」、第3水準1-88-81)\" class=\"gaiji\" src=\"../../../gaiji/1-88/1-88-81.png\"/>の赤くなつた、肉食鳥のやうな、<ruby><rb>鋭</rb><rp>（</rp><rt>するど</rt><rp>）</rp></ruby>い眼で見たのである。それから、<ruby><rb>皺</rb><rp>（</rp><rt>しは</rt><rp>）</rp></ruby>で、殆、鼻と一つになつた唇を、何か物でも<ruby><rb>噛</rb><rp>（</rp><rt>か</rt><rp>）</rp></ruby>んでゐるやうに動かした。細い喉で、尖つた<ruby><rb>喉佛</rb><rp>（</rp><rt>のどぼとけ</rt><rp>）</rp></ruby>の動いてゐるのが見える。その時、その<ruby><rb>喉</rb><rp>（</rp><rt>のど</rt><rp>）</rp></ruby>から、<ruby><rb>鴉</rb><rp>（</rp><rt>からす</rt><rp>）</rp></ruby>の啼くやうな聲が、喘ぎ喘ぎ、下人の<ruby><rb>耳</rb><rp>（</rp><rt>みゝ</rt><rp>）</rp></ruby>へ傳はつて來た。<br/>\r\n",
            "「この髮を拔いてな、この女の髮を拔いてな、<ruby><rb>鬘</rb><rp>（</rp><rt>かつら</rt><rp>）</rp></ruby>にせうと思うたのぢや。」<br/>\r\n",
            "　下人は、老婆の答が存外、<ruby><rb>平凡</rb><rp>（</rp><rt>へいぼん</rt><rp>）</rp></ruby>なのに失望した。さうして<ruby><rb>失望</rb><rp>（</rp><rt>しつばう</rt><rp>）</rp></ruby>すると同時に、又前の憎惡が、冷な<ruby><rb>侮蔑</rb><rp>（</rp><rt>ぶべつ</rt><rp>）</rp></ruby>と一しよに、心の中へはいつて來た。すると、その<ruby><rb>氣色</rb><rp>（</rp><rt>けしき</rt><rp>）</rp></ruby>が、先方へも通じたのであらう。老婆は、<ruby><rb>片手</rb><rp>（</rp><rt>かたて</rt><rp>）</rp></ruby>に、まだ屍骸の頭から<ruby><rb>奪</rb><rp>（</rp><rt>と</rt><rp>）</rp></ruby>つた長い拔け毛を<ruby><rb>持</rb><rp>（</rp><rt>も</rt><rp>）</rp></ruby>つたなり、<ruby><rb>蟇</rb><rp>（</rp><rt>ひき</rt><rp>）</rp></ruby>のつぶやくやうな聲で、口ごもりながら、こんな事を云つた。<br/>\r\n",
            "　成程、死人の<ruby><rb>髮</rb><rp>（</rp><rt>かみ</rt><rp>）</rp></ruby>の<ruby><rb>毛</rb><rp>（</rp><rt>け</rt><rp>）</rp></ruby>を拔くと云ふ事は、惡い事かも<ruby><rb>知</rb><rp>（</rp><rt>し</rt><rp>）</rp></ruby>れぬ。しかし、かう云ふ死人の多くは、皆、その位な<ruby><rb>事</rb><rp>（</rp><rt>こと</rt><rp>）</rp></ruby>を、されてもいゝ<ruby><rb>人間</rb><rp>（</rp><rt>にんげん</rt><rp>）</rp></ruby>ばかりである。現に、自分が今、<ruby><rb>髮</rb><rp>（</rp><rt>かみ</rt><rp>）</rp></ruby>を拔いた女などは、<ruby><rb>蛇</rb><rp>（</rp><rt>へび</rt><rp>）</rp></ruby>を四寸ばかりづゝに<ruby><rb>切</rb><rp>（</rp><rt>き</rt><rp>）</rp></ruby>つて干したのを、<ruby><rb>干魚</rb><rp>（</rp><rt>ほしうを</rt><rp>）</rp></ruby>だと云つて、<ruby><rb>太刀帶</rb><rp>（</rp><rt>たてはき</rt><rp>）</rp></ruby>の陣へ賣りに行つた。疫病にかゝつて死ななかつたなら、今でも賣りに行つてゐたかもしれない。しかも、この<ruby><rb>女</rb><rp>（</rp><rt>をんな</rt><rp>）</rp></ruby>の賣る干魚は、<ruby><rb>味</rb><rp>（</rp><rt>あぢ</rt><rp>）</rp></ruby>がよいと云ふので、太刀帶たちが、缺かさず<ruby><rb>菜料</rb><rp>（</rp><rt>さいれう</rt><rp>）</rp></ruby>に買つてゐたのである。自分は、この女のした事が<ruby><rb>惡</rb><rp>（</rp><rt>わる</rt><rp>）</rp></ruby>いとは思はない。しなければ、<ruby><rb>饑死</rb><rp>（</rp><rt>うゑじに</rt><rp>）</rp></ruby>をするので、<ruby><rb>仕方</rb><rp>（</rp><rt>しかた</rt><rp>）</rp></ruby>がなくした事だからである。だから、又今、<ruby><rb>自分</rb><rp>（</rp><rt>じぶん</rt><rp>）</rp></ruby>のしてゐた事も惡い事とは<ruby><rb>思</rb><rp>（</rp><rt>おも</rt><rp>）</rp></ruby>はない。これもやはりしなければ、<ruby><rb>饑死</rb><rp>（</rp><rt>うゑじに</rt><rp>）</rp></ruby>をするので、仕方がなくする事だからである。さうして、その仕方がない事を、よく知つてゐたこの女は、自分のする事を<ruby><rb>許</rb><rp>（</rp><rt>ゆる</rt><rp>）</rp></ruby>してくれるのにちがひないと<ruby><rb>思</rb><rp>（</rp><rt>おも</rt><rp>）</rp></ruby>ふからである。――老婆は、大體こんな意味の事を云つた。<br/>\r\n",
            "　下人は、太刀を<ruby><rb>鞘</rb><rp>（</rp><rt>さや</rt><rp>）</rp></ruby>におさめて、その太刀の柄を<ruby><rb>左</rb><rp>（</rp><rt>ひだり</rt><rp>）</rp></ruby>の<ruby><rb>手</rb><rp>（</rp><rt>て</rt><rp>）</rp></ruby>でおさへながら、冷然として、この話を聞いてゐた。勿論、<ruby><rb>右</rb><rp>（</rp><rt>みぎ</rt><rp>）</rp></ruby>の<ruby><rb>手</rb><rp>（</rp><rt>て</rt><rp>）</rp></ruby>では、赤く<ruby><rb>頬</rb><rp>（</rp><rt>ほゝ</rt><rp>）</rp></ruby>に<ruby><rb>膿</rb><rp>（</rp><rt>うみ</rt><rp>）</rp></ruby>を持つた大きな面皰を<ruby><rb>氣</rb><rp>（</rp><rt>き</rt><rp>）</rp></ruby>にしながら、聞いてゐるのである。しかし、之を<ruby><rb>聞</rb><rp>（</rp><rt>き</rt><rp>）</rp></ruby>いてゐる中に、下人の心には、<ruby><rb>或</rb><rp>（</rp><rt>ある</rt><rp>）</rp></ruby><ruby><rb>勇氣</rb><rp>（</rp><rt>ゆうき</rt><rp>）</rp></ruby>が生まれて來た。それは、さつき、<ruby><rb>門</rb><rp>（</rp><rt>もん</rt><rp>）</rp></ruby>の<ruby><rb>下</rb><rp>（</rp><rt>した</rt><rp>）</rp></ruby>でこの男に缺けてゐた勇氣である。さうして、<ruby><rb>又</rb><rp>（</rp><rt>また</rt><rp>）</rp></ruby>さつき、この門の<ruby><rb>上</rb><rp>（</rp><rt>うへ</rt><rp>）</rp></ruby>へ<ruby><rb>上</rb><rp>（</rp><rt>あが</rt><rp>）</rp></ruby>つて、この老婆を捕へた時の勇氣とは、<ruby><rb>全然</rb><rp>（</rp><rt>ぜん／″＼</rt><rp>）</rp></ruby>、反對な方向に<ruby><rb>動</rb><rp>（</rp><rt>うご</rt><rp>）</rp></ruby>かうとする勇氣である。下人は、饑死をするか<ruby><rb>盗人</rb><rp>（</rp><rt>ぬすびと</rt><rp>）</rp></ruby>になるかに迷はなかつたばかりではない。その<ruby><rb>時</rb><rp>（</rp><rt>とき</rt><rp>）</rp></ruby>のこの男の心もちから云へば、<ruby><rb>饑死</rb><rp>（</rp><rt>うゑじに</rt><rp>）</rp></ruby>などと云ふ事は、殆、<ruby><rb>考</rb><rp>（</rp><rt>かんが</rt><rp>）</rp></ruby>へる事さへ出來ない程、意識の外に追ひ出されてゐた。<br/>\r\n",
            "「きつと、そうか。」<br/>\r\n",
            "　老婆の話が完ると、下人は<ruby><rb>嘲</rb><rp>（</rp><rt>あざけ</rt><rp>）</rp></ruby>るやうな聲で<ruby><rb>念</rb><rp>（</rp><rt>ねん</rt><rp>）</rp></ruby>を押した。さうして、一<ruby><rb>足</rb><rp>（</rp><rt>あし</rt><rp>）</rp></ruby><ruby><rb>前</rb><rp>（</rp><rt>まへ</rt><rp>）</rp></ruby>へ出ると、<ruby><rb>不意</rb><rp>（</rp><rt>ふい</rt><rp>）</rp></ruby>に、右の手を面皰から離して、老婆の<ruby><rb>襟上</rb><rp>（</rp><rt>えりがみ</rt><rp>）</rp></ruby>をつかみながら、かう云つた。<br/>\r\n",
            "「では、己が<ruby><rb>引剥</rb><rp>（</rp><rt>ひはぎ</rt><rp>）</rp></ruby>をしようと恨むまいな。己もさうしなければ、饑死をする體なのだ。」<br/>\r\n",
            "　下人は、すばやく、老婆の<ruby><rb>着物</rb><rp>（</rp><rt>きもの</rt><rp>）</rp></ruby>を剥ぎとつた。それから、<ruby><rb>足</rb><rp>（</rp><rt>あし</rt><rp>）</rp></ruby>にしがみつかうとする老婆を、<ruby><rb>手荒</rb><rp>（</rp><rt>てあら</rt><rp>）</rp></ruby>く屍骸の上へ<ruby><rb>蹴倒</rb><rp>（</rp><rt>けたほ</rt><rp>）</rp></ruby>した。梯子の口までは、<ruby><rb>僅</rb><rp>（</rp><rt>わづか</rt><rp>）</rp></ruby>に五歩を數へるばかりである。下人は、<ruby><rb>剥</rb><rp>（</rp><rt>は</rt><rp>）</rp></ruby>ぎとつた檜肌色の<ruby><rb>着物</rb><rp>（</rp><rt>きもの</rt><rp>）</rp></ruby>をわきにかゝへて、またゝく間に急な梯子を夜の底へかけ下りた。<br/>\r\n",
            "　<ruby><rb>暫</rb><rp>（</rp><rt>しばらく</rt><rp>）</rp></ruby>、死んだやうに倒れてゐた老婆が、屍骸の<ruby><rb>中</rb><rp>（</rp><rt>なか</rt><rp>）</rp></ruby>から、その<ruby><rb>裸</rb><rp>（</rp><rt>はだか</rt><rp>）</rp></ruby>の體を起したのは、それから<ruby><rb>間</rb><rp>（</rp><rt>ま</rt><rp>）</rp></ruby>もなくの事である。老婆は、つぶやくやうな、うめくやうな聲を立てながら、まだ<ruby><rb>燃</rb><rp>（</rp><rt>も</rt><rp>）</rp></ruby>えてゐる火の光をたよりに、<ruby><rb>梯子</rb><rp>（</rp><rt>はしご</rt><rp>）</rp></ruby>の口まで、這つて行つた。さうして、そこから、短い<ruby><rb>白髮</rb><rp>（</rp><rt>しらが</rt><rp>）</rp></ruby>を倒にして、門の下を<ruby><rb>覗</rb><rp>（</rp><rt>のぞ</rt><rp>）</rp></ruby>きこんだ。外には、唯、黒洞々たる夜があるばかりである。<br/>\r\n",
            "　下人は、既に、<ruby><rb>雨</rb><rp>（</rp><rt>あめ</rt><rp>）</rp></ruby>を<ruby><rb>冐</rb><rp>（</rp><rt>をか</rt><rp>）</rp></ruby>して、京都の町へ強盗を働きに急いでゐた。<br/>\n",
            "<div class=\"chitsuki_2\" style=\"text-align:right; margin-right: 2em\">――四年九月――</div>\n",
            "<br/>\n",
            "<br/>\n",
            "<br/>\n",
            "</div>\n",
            "<div class=\"bibliographical_information\">\n",
            "<hr/>\n",
            "<br/>\r\n",
            "底本：「新選　名著復刻全集　近代文学館　芥川龍之介著　羅生門　阿蘭陀書房版」ほるぷ出版<br/>\r\n",
            "　　　1976（昭和51）年4月1日発行<br/>\r\n",
            "※疑問点の確認にあたっては、「日本の文学33　羅生門」ほるぷ出版、1984（昭和59）年8月1日初版第1刷発行を参照しました。<br/>\r\n",
            "入力：j.utiyama<br/>\r\n",
            "校正：もりみつじゅんじ、野口英司<br/>\r\n",
            "1999年6月9日公開<br/>\r\n",
            "2010年11月4日修正<br/>\r\n",
            "青空文庫作成ファイル：<br/>\r\n",
            "このファイルは、インターネットの図書館、<a href=\"http://www.aozora.gr.jp/\">青空文庫（http://www.aozora.gr.jp/）</a>で作られました。入力、校正、制作にあたったのは、ボランティアの皆さんです。<br/>\n",
            "<br/>\n",
            "<br/>\n",
            "</div>\n",
            "<div class=\"notation_notes\">\n",
            "<hr/>\n",
            "<br/>\r\n",
            "●表記について<br/>\n",
            "<ul>\n",
            "<li>このファイルは W3C 勧告 XHTML1.1 にそった形式で作成されています。</li>\n",
            "<li>［＃…］は、入力者による注を表す記号です。</li>\n",
            "<li>「くの字点」をのぞくJIS X 0213にある文字は、画像化して埋め込みました。</li>\n",
            "</ul>\n",
            "</div>\n",
            "<div id=\"card\">\n",
            "<hr/>\n",
            "<br/>\n",
            "<a href=\"JavaScript:goLibCard();\" id=\"goAZLibCard\">●図書カード</a><script src=\"../../contents.js\" type=\"text/javascript\"></script>\n",
            "<script src=\"../../golibcard.js\" type=\"text/javascript\"></script>\n",
            "</div></body>\n",
            "</html>\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ストップワードの除去"
      ],
      "metadata": {
        "id": "_qfoCpI5SPlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **ストップワードリストの取得とクリーニング**:\n",
        "   - ストップワードリストの取得例:"
      ],
      "metadata": {
        "id": "4sAXCAIUSQpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib import request\n",
        "\n",
        "url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
        "response = request.urlopen(url)\n",
        "soup = BeautifulSoup(response)\n",
        "response.close()\n",
        "\n",
        "stopwords_text = soup.text\n",
        "stopwords_list = stopwords_text.split(\"\\r\\n\")\n",
        "stopwords_list = [word for word in stopwords_list if word]\n",
        "print(stopwords_list)\n",
        "\n",
        "#     - このコードは、ストップワードのリストをWebから取得し、クリーニングしています。\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTSO-U4JSTVP",
        "outputId": "84a9fcbd-716a-4696-875e-a595f1cd6e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['あそこ', 'あたり', 'あちら', 'あっち', 'あと', 'あな', 'あなた', 'あれ', 'いくつ', 'いつ', 'いま', 'いや', 'いろいろ', 'うち', 'おおまか', 'おまえ', 'おれ', 'がい', 'かく', 'かたち', 'かやの', 'から', 'がら', 'きた', 'くせ', 'ここ', 'こっち', 'こと', 'ごと', 'こちら', 'ごっちゃ', 'これ', 'これら', 'ごろ', 'さまざま', 'さらい', 'さん', 'しかた', 'しよう', 'すか', 'ずつ', 'すね', 'すべて', 'ぜんぶ', 'そう', 'そこ', 'そちら', 'そっち', 'そで', 'それ', 'それぞれ', 'それなり', 'たくさん', 'たち', 'たび', 'ため', 'だめ', 'ちゃ', 'ちゃん', 'てん', 'とおり', 'とき', 'どこ', 'どこか', 'ところ', 'どちら', 'どっか', 'どっち', 'どれ', 'なか', 'なかば', 'なに', 'など', 'なん', 'はじめ', 'はず', 'はるか', 'ひと', 'ひとつ', 'ふく', 'ぶり', 'べつ', 'へん', 'ぺん', 'ほう', 'ほか', 'まさ', 'まし', 'まとも', 'まま', 'みたい', 'みつ', 'みなさん', 'みんな', 'もと', 'もの', 'もん', 'やつ', 'よう', 'よそ', 'わけ', 'わたし', 'ハイ', '上', '中', '下', '字', '年', '月', '日', '時', '分', '秒', '週', '火', '水', '木', '金', '土', '国', '都', '道', '府', '県', '市', '区', '町', '村', '各', '第', '方', '何', '的', '度', '文', '者', '性', '体', '人', '他', '今', '部', '課', '係', '外', '類', '達', '気', '室', '口', '誰', '用', '界', '会', '首', '男', '女', '別', '話', '私', '屋', '店', '家', '場', '等', '見', '際', '観', '段', '略', '例', '系', '論', '形', '間', '地', '員', '線', '点', '書', '品', '力', '法', '感', '作', '元', '手', '数', '彼', '彼女', '子', '内', '楽', '喜', '怒', '哀', '輪', '頃', '化', '境', '俺', '奴', '高', '校', '婦', '伸', '紀', '誌', 'レ', '行', '列', '事', '士', '台', '集', '様', '所', '歴', '器', '名', '情', '連', '毎', '式', '簿', '回', '匹', '個', '席', '束', '歳', '目', '通', '面', '円', '玉', '枚', '前', '後', '左', '右', '次', '先', '春', '夏', '秋', '冬', '一', '二', '三', '四', '五', '六', '七', '八', '九', '十', '百', '千', '万', '億', '兆', '下記', '上記', '時間', '今回', '前回', '場合', '一つ', '年生', '自分', 'ヶ所', 'ヵ所', 'カ所', '箇所', 'ヶ月', 'ヵ月', 'カ月', '箇月', '名前', '本当', '確か', '時点', '全部', '関係', '近く', '方法', '我々', '違い', '多く', '扱い', '新た', 'その後', '半ば', '結局', '様々', '以前', '以後', '以降', '未満', '以上', '以下', '幾つ', '毎日', '自体', '向こう', '何人', '手段', '同じ', '感じ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **実際の文章からストップワード除去**:\n",
        "   - ストップワード除去の例:"
      ],
      "metadata": {
        "id": "CS4g-gdoSZ0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_text_list =  ['私', 'は', '今日', '、', 'スーパー', 'で', '沢山', 'の', 'お', '菓子', 'を', '買っ', 'た', '。']\n",
        "result_text_list = list()\n",
        "for split_text in split_text_list:\n",
        "  if split_text not in stopwords_list:\n",
        "    result_text_list.append(split_text)\n",
        "\n",
        "print(result_text_list)\n",
        "\n",
        "#     - このコードは、分割された単語リストからストップワードを除去しています。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQuCWylxSbia",
        "outputId": "0abc4da2-333a-4929-848a-78428f89e7a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['は', '今日', '、', 'スーパー', 'で', '沢山', 'の', 'お', '菓子', 'を', '買っ', 'た', '。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> これらのコード例は、自然言語処理におけるテキストの前処理を行う際の基本的な手法を示しています。教材に基づいて、さまざまな前処理手法を学び、それらを実際のプログラミングに適用することができます。\n",
        "\n"
      ],
      "metadata": {
        "id": "M2xUypJqSi6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 課題：スクレイピングしたテキストの前処理"
      ],
      "metadata": {
        "id": "Qjwo6DLhSyCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 夏目漱石の『文壇の趨勢』を青空文庫からBeautiful Soup4と正規表現モジュールreを使用してスクレイピングし、前処理する。\n",
        "2. HTMLタグや不要な文字を削除し、最初の文を手動で品詞ごとに区切り、ストップワードを除去する。\n",
        "3. 自然言語処理におけるデータ前処理の技術を理解し、適用する。"
      ],
      "metadata": {
        "id": "HX2TXep_T8WZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "このPython課題について、まずはヒントを箇条書きで、その後模範回答を示します。\n",
        "\n",
        "### ヒント\n",
        "1. **Beautiful Soupを使用する**: このライブラリはHTMLやXMLファイルからデータを抽出するために使います。特定のURLからデータを取得し、解析するのに役立ちます。\n",
        "2. **正規表現モジュール(re)の使用**: テキストから特定のパターン（例えばHTMLタグ）を見つけ出し、除去するのに便利です。\n",
        "3. **HTMLタグの除去**: BeautifulSoupで取得したテキストから、HTMLタグを取り除くために`.get_text()`メソッドや正規表現を使用します。\n",
        "4. **ストップワードの除去**: ストップワードとは、一般的に意味を持たない単語（例: \"と\", \"は\", \"の\"）のことです。これらをテキストから取り除くことで、分析をより効果的に行えます。\n",
        "5. **品詞ごとに区切る**: 特定の文を手動で品詞ごとに区切る場合、日本語の形態素解析ツール（例: MeCab）を使用することが考えられますが、この課題では手動で行います。\n",
        "\n",
        "### 模範回答\n",
        "以下は、この課題に対する模範回答のコード例です。このコードはBeautiful Soupを使って特定のウェブページからテキストをスクレイピングし、その後HTMLタグやストップワードを除去します。\n",
        "\n"
      ],
      "metadata": {
        "id": "r0yNn0vaTmJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# スクレイピングするURL\n",
        "url = 'https://www.aozora.gr.jp/cards/000148/files/2371_13943.html'\n",
        "\n",
        "# URLからHTMLを取得\n",
        "response = requests.get(url)\n",
        "response.encoding = response.apparent_encoding\n",
        "\n",
        "# BeautifulSoupでHTMLを解析\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# 本文を取得し、HTMLタグを除去\n",
        "text = soup.get_text()\n",
        "\n",
        "# 不要な文字列や改行を削除\n",
        "text = re.sub(r'\\n|\\r', '', text)\n",
        "text = re.sub(r'［＃.*?］', '', text)  # 注釈などの特別なタグを除去\n",
        "\n",
        "# ここで、手動で最初の文を品詞ごとに区切り、ストップワードを除去する処理を行う\n",
        "# 例: \"近頃は文壇の趨勢がどうのこうのという話題になることが少ない。\" -> \"近頃 文壇 趨勢 どうのこうの 話題 なる 少ない。\"\n",
        "\n",
        "# 結果の表示\n",
        "print(text[:500])  # 最初の500文字を表示"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVXjsJilTqXy",
        "outputId": "59465840-12b2-4200-f955-212c035da8e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "夏目漱石 文壇の趨勢文壇の趨勢夏目漱石　近頃は大分方々の雑誌から談話をしろしろと責められて、頭ががらん胴になったから、当分品切れの看板でも懸けたいくらいに思っています。現に今日も一軒断わりました。向後日本の文壇はどう変化するかなどという大問題はなかなか分りにくい。いわんや二三日前まで『文学評論』の訂正をしていて、頭が痺れたように疲れているから、早速に分別も浮びません。それに似寄った事をせんだってごく簡略に『秀才文壇』の人に話してしまった。あいにくこの方面も種切れです。が、まあせっかくだから――いつおいでになっても、私の談話が御役に立った試がないようだから――つまらん事でも責任逃れに話しましょう。　私が小説を書き出したのは、何年前からか確と覚えてもいないが、けっして古くはない。見方によればごく近頃であると云ってもよろしい。しかるに我が文壇の潮流は非常に急なもので、私よりあとから、小説家として、世にあらわれ、また一般から作家として認められたものが大分ある。今も続々出つつあるように思われる。私は多忙な身だから、ほかの人の作を一々通読する暇がない。たてこんで来ると、つい読み損って、それぎりにす\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "このコードは、指定されたウェブページからテキストを取得し、HTMLタグを除去した後、最初の500文字を表示します。なお、手動での品詞分割とストップワードの除去は、このコードでは実装されていません。実際の実装では、テキストデータに応じて適切な方法を選択する必要があります。"
      ],
      "metadata": {
        "id": "XoT8dCBOTt1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vecで単語間の類似度を測定してみよう"
      ],
      "metadata": {
        "id": "aghGwXTuVE8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 教材要約（文語調、箇条書き）\n",
        "- **目標**:\n",
        "  - 分散表現の理解。\n",
        "  - 分散表現の用途を理解する。\n",
        "  - Word2Vecなどのライブラリや手法を使いこなす。\n",
        "\n",
        "- **分散表現**:\n",
        "  - 単語を数百次元のベクトルで表す。\n",
        "  - one-hotエンコーディングの欠点を解決。\n",
        "  - 単語間の意味的な関係を表現可能。\n",
        "\n",
        "- **Word2Vec**:\n",
        "  - 単語の分散表現を学習するモデル。\n",
        "  - Skip-gram、CBOWモデルを使用。\n",
        "  - 類似単語の特定や関係性の理解が可能。\n",
        "\n",
        "- **gensimライブラリ**:\n",
        "  - Word2VecのPython実装。\n",
        "  - コーパスの読み込みとモデル学習をサポート。\n",
        "\n",
        "- **cos類似度**:\n",
        "  - ベクトル間の類似度を計測する方法。\n",
        "  - 0から1の範囲で類似度を測定。\n",
        "\n",
        "本章では、自然言語処理における分散表現とその学習モデルであるWord2Vecについて学びます。分散表現は単語を多次元のベクトルで表す技術で、単語間の意味的な関係を捉えることが可能です。Word2Vecはこの分散表現を効果的に学習し、単語の類似性や関係を解析することができます。gensimライブラリを用いてWord2Vecモデルの実装と学習が行われ、cos類似度を使用して単語間の類似度を計測します。この章を通じて、単語の意味的な関係を理解し、それを用いた自然言語処理の応用が可能となります。\n",
        "\n",
        "---\n",
        "＜口語要約＞\n",
        "この章では、Word2Vecというツールについて学びます。Word2Vecを使うと、単語をベクトル（つまり数字の羅列）で表現できて、単語同士がどれくらい似ているかとか、どんな関係があるかとかが分かるようになる。例えば、「王様」から「男」を引いて「女」を足すと「女王」が出てくる、みたいな計算が可能です。\n",
        "\n",
        "このWord2Vec、中でどうやってるかというと、単語の周りにどんな単語があるか見て、その単語の「意味」を数値で表現している。これを「分散表現」と呼ぶ。gensimっていうPythonのライブラリを使うと、Word2Vecを簡単に使えるようになり、どの単語が似ているかを計算したり、単語の「意味」を数値で理解できる。\n",
        "\n",
        "Word2Vecは自然言語処理の世界でとても重要で、文章や単語の意味をコンピューターに理解させるのに役立つツールです。\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kGrTOUx1VJ5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### gensimライブラリの使用方法\n",
        "\n",
        "1. **gensimのインストール**:\n",
        "   - gensimをインストールするためのコード:"
      ],
      "metadata": {
        "id": "08HqOKzRVW1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1uaUv_bVZnI",
        "outputId": "a76910ea-ca15-4fa1-d41e-caf0aa7df760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **コーパスの読み込み**:\n",
        "   - 英語のコーパス`text8`をgensimを使って読み込むコード:"
      ],
      "metadata": {
        "id": "xcG1ISuSVeDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "corpus = api.load(\"text8\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OWhzg6w8Vhfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "709ea8dc-edeb-4637-d475-dc1213e051e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - コーパスの情報を確認するコード:"
      ],
      "metadata": {
        "id": "hMsx7y7FVsXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "api.info(\"text8\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs5l7CCFVnQW",
        "outputId": "1f7889d5-627a-4b37-86a5-c3df5a04156d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_records': 1701,\n",
              " 'record_format': 'list of str (tokens)',\n",
              " 'file_size': 33182058,\n",
              " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
              " 'license': 'not found',\n",
              " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
              " 'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
              " 'file_name': 'text8.gz',\n",
              " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
              " 'parts': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Word2Vecモデルの学習**:\n",
        "   - `text8`コーパスを使用してWord2Vecモデルを学習するコード:\n"
      ],
      "metadata": {
        "id": "p3Tndu6kVyCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(corpus)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDrLja_yVzld",
        "outputId": "949f1ba9-4555-456a-a787-1e5c3c453932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=71290, vector_size=100, alpha=0.025>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 単語間の類似度測定\n",
        "\n",
        "1. **単語間の類似度を測定**:\n",
        "   - 例えば「japan」という単語に対して、類似度が高い単語を求めるコード:"
      ],
      "metadata": {
        "id": "n27S0vjgV2gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_words = model.wv.most_similar(\"japan\")\n",
        "print(similarity_words)"
      ],
      "metadata": {
        "id": "7ItZ56WLV5RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - 学習時に使用した単語を確認するコード:"
      ],
      "metadata": {
        "id": "4AuFwFluV8g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.wv.index_to_key)"
      ],
      "metadata": {
        "id": "f3VLM01cV9ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "     - Gensimのバージョンが4.0.0未満の場合は、以下のコードを使用:"
      ],
      "metadata": {
        "id": "vrypMMfeWAmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.wv.index2entity)\n",
        "\n"
      ],
      "metadata": {
        "id": "gesxkv4GWCQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> これらのコードは、Word2Vecモデルの基本的な使い方を示しています。gensimライブラリを使って、英語コーパスを読み込み、Word2Vecモデルを学習させることができます。また、学習したモデルを使用して、特定の単語に類似した単語を見つけることができます。これにより、単語間の意味的な関連性を分析することが可能になります。\n",
        "\n"
      ],
      "metadata": {
        "id": "BCtdQTVFWFRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MeCabでワードクラウドを作ろう"
      ],
      "metadata": {
        "id": "K3lk6khoWwKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 教材要約（文語調、箇条書き）\n",
        "- **目標**:\n",
        "  - 形態素解析の理解。\n",
        "  - 形態素解析の用途を把握する。\n",
        "  - 形態素解析に用いるMeCabの使い方を習得する。\n",
        "\n",
        "- **形態素解析**:\n",
        "  - 文章を意味を持つ最小単位に分割する技術。\n",
        "  - 各形態素の品詞を特定し、解析する。\n",
        "\n",
        "- **MeCab**:\n",
        "  - 日本語形態素解析エンジン。\n",
        "  - テキストを品詞ごとに分割し、辞書情報に基づいて分析する。\n",
        "\n",
        "- **TF-IDF**:\n",
        "  - 文書内の各単語の重要度を計算する技術。\n",
        "  - 単語の出現頻度と希少性を数値化する。\n",
        "\n",
        "- **ワードクラウド**:\n",
        "  - 文書の主要な単語を視覚的に表現する方法。\n",
        "  - 出現頻度の高い単語を目立たせて表示する。\n",
        "\n",
        "#### 明解な要約文章（文語調）\n",
        "本章では、日本語の形態素解析に焦点を当て、MeCabを用いた分析手法について学習します。形態素解析は、文章を意味を持つ最小の単位に分割し、各単語の品詞や特性を分析する技術です。この技術を活用することで、文章の詳細な解析や理解が可能になります。MeCabはこの形態素解析を効率的に行うためのツールであり、テキストデータを詳細に分析する際に欠かせないものです。また、TF-IDFを用いて単語の重要度を計算し、ワードクラウドを生成することで、文書のキーワードや主要なテーマを視覚化できます。これらの技術は、テキストデータの分析や処理において重要な役割を果たします。"
      ],
      "metadata": {
        "id": "1RJeg4owYagQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 教材要約（口語調、抽象化）\n",
        "\n",
        "この章ではMeCabを使ってワードクラウドを作る方法を学ぶよ。まず、形態素解析ってのを理解することが大事。形態素解析っていうのは、文章を単語や品詞に分ける技術のこと。これを使えば、文書の中でどんな単語がどれくらい使われているかが分かるんだ。\n",
        "\n",
        "で、MeCabっていうのは、形態素解析をするためのツール。これを使うと、文章を細かく分析できて、どの単語が多いか、どの品詞が多いか、そういうのがすぐに分かるようになる。それで、TF-IDFっていうのを使って、単語の重要度を数値化するんだ。これがあると、文書の中で本当に大事な単語が何かが分かる。\n",
        "\n",
        "最後に、ワードクラウドっていうのを作るんだけど、これは出現頻度の高い単語を目立たせて視覚的に表示する方法。結構面白いよ。文章の中でどの単語が強調されるか一目で分かるし、デザイン的にもカッコいい。\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GBlstAfZW1Vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MeCabを用いた形態素解析（コード集）\n",
        "\n",
        "\n",
        "1. **MeCabのインストール**:\n",
        "   - PythonでMeCabを利用するためのインストールコード:"
      ],
      "metadata": {
        "id": "g2bmPFDVXb6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mecab-python3"
      ],
      "metadata": {
        "id": "5iZAJjDVXirw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0efefc5b-51dc-4e8b-aed8-efac0e2b0f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mecab-python3\n",
            "  Downloading mecab_python3-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/581.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/581.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m430.1/581.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.7/581.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **辞書のインストール**:\n",
        "   - MeCabで使う辞書をインストールするコード:"
      ],
      "metadata": {
        "id": "HSqM86LkXlwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidic-lite"
      ],
      "metadata": {
        "id": "Tfz1S1WdXm6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **形態素解析の実行**:\n",
        "   - テキストを形態素解析するコード:"
      ],
      "metadata": {
        "id": "vttuTZQUXoyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "mecab_tagger = MeCab.Tagger()\n",
        "text = \"私は今日、スーパーで沢山のお菓子を買った。\"\n",
        "print(mecab_tagger.parse(text))"
      ],
      "metadata": {
        "id": "DtujVkRLXpr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**文章中の単語の出現頻度を確認してみよう**"
      ],
      "metadata": {
        "id": "BgbjO82-vUfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "mecab_tagger = MeCab.Tagger()\n",
        "\n",
        "text = \"私は今日、スーパーで沢山のお菓子を買った。\"\n",
        "node = mecab_tagger.parseToNode(text)\n",
        "print(node)"
      ],
      "metadata": {
        "id": "RxIvokeXvLxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に形態素（単語）、品詞を取得する方法として、以下の属性を使用します。\n",
        "\n",
        "\n",
        "\n",
        "> surface: 形態素（単語）\n",
        "> posid: 品詞\n",
        "> feature: 詳細情報\n",
        "\n"
      ],
      "metadata": {
        "id": "8EykdG9rviju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "mecab_tagger = MeCab.Tagger()\n",
        "\n",
        "text = \"私は今日、スーパーで沢山のお菓子を買った。\"\n",
        "node = mecab_tagger.parseToNode(text)\n",
        "\n",
        "while node:\n",
        "    # 単語、品詞、詳細情報をタブ区切りで表示\n",
        "    print(f'{node.surface}\\t{node.posid}\\t{node.feature}')\n",
        "    # 次の要素を取得\n",
        "    node = node.next"
      ],
      "metadata": {
        "id": "jZRxQIEhvdB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**単語の出現頻度の確認**"
      ],
      "metadata": {
        "id": "fXkcKnRkvwO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> wikipediaにある自然言語処理の説明内容を使用し、MeCabで形態素解析します。\n",
        "\n"
      ],
      "metadata": {
        "id": "7ETrH6Svv4F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "mecab_tagger = MeCab.Tagger()\n",
        "\n",
        "text = '''自然言語処理（しぜんげんごしょり、英語: natural language processing、略称：NLP）は、\n",
        "人間が日常的に使っている自然言語をコンピュータに処理させる一連の技術であり、人工知能と言語学の\n",
        "一分野である。「計算言語学」（computational linguistics）との類似もあるが、自然言語処理は工学的\n",
        "な視点からの言語処理をさすのに対して、計算言語学は言語学的視点を重視する手法をさす事が多い[1]。\n",
        "データベース内の情報を自然言語に変換したり、自然言語の文章をより形式的な（コンピュータが理解し\n",
        "やすい）表現に変換するといった処理が含まれる。応用例としては予測変換、IMEなどの文字変換が挙げら\n",
        "れる。自然言語の理解をコンピュータにさせることは、自然言語理解とされている。自然言語理解と、自\n",
        "然言語処理の差は、意味を扱うか、扱わないかという説もあったが、最近は数理的な言語解析手法（統計\n",
        "や確率など）が広められた為、パーサ（統語解析器）などの精度や速度が一段と上がり、その意味合いは\n",
        "違ってきている。もともと自然言語の意味論的側面を全く無視して達成できることは非常に限られている。\n",
        "このため、自然言語処理には形態素解析と構文解析、文脈解析、意味解析などをSyntaxなど表層的な観点\n",
        "から解析をする学問であるが、自然言語理解は、意味をどのように理解するかという個々人の理解と推論\n",
        "部分が主な研究の課題になってきており、両者の境界は意思や意図が含まれるかどうかになってきている。'''\n",
        "node = mecab_tagger.parseToNode(text)\n",
        "count_dict = {}\n",
        "\n",
        "while node:\n",
        "    word = node.surface\n",
        "    hinshi = node.feature.split(\",\")[0]\n",
        "    if word in count_dict.keys() and hinshi == \"名詞\":\n",
        "        count_dict[word] += 1\n",
        "    elif hinshi == \"名詞\":\n",
        "        count_dict[word] = 1\n",
        "    else:\n",
        "        pass\n",
        "    node = node.next\n",
        "\n",
        "word_counts = sorted(count_dict.items(), key=lambda x:x[1], reverse=True)\n",
        "word_counts"
      ],
      "metadata": {
        "id": "f_NAe5JLvx6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Guooy4q3vxWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TF-IDFを利用したワードクラウドの作成\n",
        "\n",
        "1. **TF-IDFの計算**:\n",
        "   - sklearnのTF-IDFベクトルライザーを使ったTF-IDF値計算のコード:"
      ],
      "metadata": {
        "id": "hJWRxTjCXvTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import MeCab\n",
        "\n",
        "mecab_tagger = MeCab.Tagger()\n",
        "\n",
        "text = '''自然言語処理（しぜんげんごしょり、英語: natural language processing、略称：NLP）は、\n",
        "人間が日常的に使っている自然言語をコンピュータに処理させる一連の技術であり、人工知能と言語学の\n",
        "一分野である。「計算言語学」（computational linguistics）との類似もあるが、自然言語処理は工学的\n",
        "な視点からの言語処理をさすのに対して、計算言語学は言語学的視点を重視する手法をさす事が多い[1]。\n",
        "データベース内の情報を自然言語に変換したり、自然言語の文章をより形式的な（コンピュータが理解し\n",
        "やすい）表現に変換するといった処理が含まれる。応用例としては予測変換、IMEなどの文字変換が挙げら\n",
        "れる。自然言語の理解をコンピュータにさせることは、自然言語理解とされている。自然言語理解と、自\n",
        "然言語処理の差は、意味を扱うか、扱わないかという説もあったが、最近は数理的な言語解析手法（統計\n",
        "や確率など）が広められた為、パーサ（統語解析器）などの精度や速度が一段と上がり、その意味合いは\n",
        "違ってきている。もともと自然言語の意味論的側面を全く無視して達成できることは非常に限られている。\n",
        "このため、自然言語処理には形態素解析と構文解析、文脈解析、意味解析などをSyntaxなど表層的な観点\n",
        "から解析をする学問であるが、自然言語理解は、意味をどのように理解するかという個々人の理解と推論\n",
        "部分が主な研究の課題になってきており、両者の境界は意思や意図が含まれるかどうかになってきている。'''\n",
        "node = mecab_tagger.parseToNode(text)\n",
        "vocab_list = []\n",
        "\n",
        "while node:\n",
        "    word = node.surface\n",
        "    hinshi = node.feature.split(\",\")[0]\n",
        "    if hinshi == \"名詞\":\n",
        "        if (not word.isnumeric()) and (not re.match(r'^[\\u3040-\\u309F]+$', word)):\n",
        "            # 名詞が数値と平仮名のみの場合は除き、それ以外の名詞を保存\n",
        "            vocab_list.append(word)\n",
        "    else:\n",
        "        pass\n",
        "    node = node.next\n",
        "\n",
        "print(vocab_list)"
      ],
      "metadata": {
        "id": "R-fQoUAYwgjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "id": "3X--Hcwxwlk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_model = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', norm=None)\n",
        "tfidf_model.fit(vocab_list)"
      ],
      "metadata": {
        "id": "Tz0oSqyhXwAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **ワードクラウドの作成**:\n",
        "   - TF-IDF値に基づいてワードクラウドを作成するコード:"
      ],
      "metadata": {
        "id": "KS_Xm6ZiXzGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 対象のテキストをtf-idf値に変換\n",
        "vocab_text = \" \".join(vocab_list)\n",
        "tfidf_vec = tfidf_model.transform([vocab_text]).toarray()[0]\n",
        "# 単語: tf-idf値となるdictに変換\n",
        "tfidf_dict = dict(zip(tfidf_model.get_feature_names_out(), tfidf_vec))\n",
        "# tf-idf値が正のみの単語を残す\n",
        "tfidf_dict = {word: num_val for word, num_val in tfidf_dict.items() if num_val > 0}\n",
        "\n",
        "tfidf_dict"
      ],
      "metadata": {
        "id": "n22Y60Arxe8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud\n",
        "!pip install matplotlib\n"
      ],
      "metadata": {
        "id": "vm_lPwcYzX5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -y install fonts-ipafont-gothic\n"
      ],
      "metadata": {
        "id": "Cbc-UJcMzebT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "font_path = \"/usr/share/fonts/truetype/fonts-japanese-gothic.ttf\"\n",
        "wc = WordCloud(background_color=\"white\",width=900, height=500, font_path=font_path).generate_from_frequencies(tfidf_dict)\n",
        "plt.figure(figsize=(18,10))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "yU31nRI_X0k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> これらのコードは、MeCabを使用して日本語テキストを形態素解析し、その結果を基にTF-IDF値を計算して、ワードクラウドを作成する方法を示しています。MeCabによる形態素解析は日本語テキストの解析に不可欠で、TF-IDFによる重要語の抽出とワードクラウドの生成は、文書のキーワードを視覚化し理解するのに有用です。\n",
        "\n"
      ],
      "metadata": {
        "id": "6vxMYxsrX4tR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spaCyを使って固有表現抽出をしてみよう"
      ],
      "metadata": {
        "id": "FwFHMVPa0fWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 教材要約（文語調、箇条書き）\n",
        "- **目標**:\n",
        "  - 固有表現抽出の理解。\n",
        "  - 固有表現抽出の用途把握。\n",
        "  - 固有表現抽出に用いるspaCyの使い方習得。\n",
        "\n",
        "- **固有表現抽出の概要**:\n",
        "  - 文章から特定の情報（地名、人名、日付など）を機械的に識別、抽出。\n",
        "  - 情報抽出の一環として位置付けられる。\n",
        "\n",
        "- **spaCyの特徴**:\n",
        "  - Pythonの自然言語処理ライブラリ。\n",
        "  - 学習済みの統計モデルと単語ベクトルを含む。\n",
        "  - 多言語に対応。\n",
        "\n",
        "- **spaCyを用いた固有表現抽出の実践**:\n",
        "  - 日本語モデルGINZAの使用。\n",
        "  - テキストデータの形態素解析と固有表現の識別。\n",
        "\n",
        "#### 明解な要約文章（文語調）\n",
        "本章では、固有表現抽出の技術とその実践に焦点を当てています。固有表現抽出は、文章から特定の名詞や日付などの情報を自動で識別し抽出するプロセスです。この技術は、テキストデータから重要な情報を効率的に得るために重要です。spaCyというPythonライブラリを用いることで、この固有表現抽出を実現します。spaCyは多言語に対応し、豊富な機能を備えており、特に固有表現抽出では顕著な能力を示します。日本語のテキストに対しても、GINZAモデルを通じて効果的な固有表現抽出が可能です。この章では、spaCyを用いた固有表現抽出の方法と、その応用事例について学習し、実践的なスキルを身につけることができます。\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0AeDDtQC0gDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 口語調の要約\n",
        "\n",
        "固有表現抽出っていうのは、文章から特定の情報を探し出す技術です。たとえば、人の名前や場所、日付とかがそれに当たります。これができると、文章から必要な情報をサッと取り出せるようになるんです。\n",
        "\n",
        "今回の教材では、spaCyというツールを使って固有表現抽出をしてみます。spaCyはPythonで使える自然言語処理ライブラリで、事前に学習させたモデルがあるんです。これを使えば、固有表現抽出がかなり簡単にできます。\n",
        "\n",
        "日本語の文章を扱う時は、GINZAというモデルを使うんですけど、これもspaCyで簡単に使えるんです。実際にコードを書いてみると、固有表現がキレイに抜き出されて、その便利さがよく分かります。\n",
        "\n",
        "この章で、spaCyを使った固有表現抽出のやり方を学ぶんですが、これができると、文章からの情報抽出がぐっとラクになります。\n"
      ],
      "metadata": {
        "id": "Oj51iGCK1uQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### コード例とその説明\n",
        "\n",
        "- **spaCyのインストールとモデルの読み込み**:"
      ],
      "metadata": {
        "id": "O9V-pQKG3RG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "print(nlp)\n",
        "\n",
        "#  - spaCyライブラリをインポートし、英語の学習済みモデルをロード。"
      ],
      "metadata": {
        "id": "q3YFFo_X3XCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **GINZAのインストールと日本語モデルの読み込み**:"
      ],
      "metadata": {
        "id": "WZ1aGiLr3lvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ja_ginza"
      ],
      "metadata": {
        "id": "rhcH6kuH3oW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ja_core_news_sm"
      ],
      "metadata": {
        "id": "KnD_m73a5t4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('ja_core_news_sm')\n",
        "\n",
        "\n",
        "#- GINZAライブラリをインストールし、日本語モデルをロード。"
      ],
      "metadata": {
        "id": "Q5dtH6Wi3rNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **形態素解析の実行**:"
      ],
      "metadata": {
        "id": "sASAOsWc3u-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "      プログラムは、高速処理ができます。...\n",
        "      まさに、プログラムならではの「できること」だといえるでしょう。\n",
        "    \"\"\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "  print(token.text, type(token))\n",
        "\n",
        "#   - 日本語のテキストに対して形態素解析を実行し、トークンごとに出力。"
      ],
      "metadata": {
        "id": "bQ-sGb1c3zFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **固有表現抽出の実行**:"
      ],
      "metadata": {
        "id": "57tcLd7U35jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"ent\", options={\"compact\":True},  jupyter=True)\n",
        "\n",
        "#  - 解析された文章から固有表現を抽出し、ビジュアル化。"
      ],
      "metadata": {
        "id": "v4rNzoN538Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">この章では、spaCyとGINZAを利用して、日本語のテキストから固有表現を抽出する方法を学びます。まず、spaCyの基本的な使い方を理解し、英語のモデルをロードしてみます。続いて、日本語処理のためにGINZAライブラリをインストールし、日本語モデルをロードします。実際に日本語のテキストデータに対して形態素解析を行い、トークンごとに内容を確認します。最後に、displacyモジュールを用いて固有表現を視覚的に表示し、テキスト内の重要な情報を抽出するプロセスを実践します。\n"
      ],
      "metadata": {
        "id": "CX1fOppq4ECI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transformersでBertのモデルを使ってみよう"
      ],
      "metadata": {
        "id": "0WijTEwR61y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**本章の目標**\n",
        "- transformersとBERTの理解\n",
        "- transformersの使い方の習得\n",
        "\n",
        "**transformersとは**\n",
        "- Hugging Face社が提供する自然言語処理向けのライブラリ\n",
        "- Jax, PyTorch, TensorFlowをバックエンドに選択可能\n",
        "- 95種類のモデルアーキテクチャをサポート\n",
        "\n",
        "**BERTとは**\n",
        "- Googleが2018年に発表した自然言語処理モデル\n",
        "- 両方向のエンコード表現を使用し、高い精度を実現\n",
        "- 様々な自然言語処理タスクで優れた性能を発揮\n",
        "\n",
        "**transformersの利用方法**\n",
        "- PythonのAPIを通して簡単に使用可能\n",
        "- パイプラインやトークナイザーを提供\n",
        "- 感情分析などのタスクに対応\n",
        "\n",
        "**感情分析の活用**\n",
        "- SNS投稿、商品レビュー、問い合わせ履歴などの分析\n",
        "- テキスト、音声、映像を含む様々なデータに適用可能\n",
        "\n",
        "**BERTでの感情分析**\n",
        "- 学習済みの日本語モデルを使用して簡単に感情分析が実施可能\n",
        "- 肯定的および否定的なテキストの正確な分析\n",
        "\n",
        "口語調の要約\n",
        "\n",
        "transformersっていうのは、自然言語処理を扱うためのライブラリですね。Googleが作ったBERTモデルも使えるんですよ。このBERT、すごくて、文章の意味を深く理解できるんです。\n",
        "\n",
        "transformersは、テキスト分類や感情分析、翻訳など、いろいろなことができます。使い方も簡単で、学習済みのモデルをダウンロードして、すぐに使えます。感情分析なんかも、日本語の文章でやってみることができますよ。\n",
        "\n",
        "transformersのおかげで、文章やSNSの投稿、商品レビューなど、さまざまなテキストから、人の感情を読み取ることが可能になりました。サポートセンターや金融機関、マーケティングなど、いろんな場面で役立てられています。\n",
        "\n",
        "BERTを使えば、文章の肯定的な感情や否定的な感情を見分けることもできます。感情分析の例としては、「嬉しい」って言葉にはポジティブなラベルが、逆に「悲しい」にはネガティブなラベルがつきました。\n",
        "\n"
      ],
      "metadata": {
        "id": "9Y2FJtrM62j9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transformersをインストール\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "B4Qm9QEQ7MvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTで感情分析を行うためのライブラリをインストール\n",
        "!pip install fugashi ipadic"
      ],
      "metadata": {
        "id": "lJgxrJ0w7Mow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pipelineを使用してBERTモデルでの感情分析\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\n",
        "    task=\"sentiment-analysis\",\n",
        "    model=\"koheiduck/bert-japanese-finetuned-sentiment\",\n",
        "    tokenizer=\"koheiduck/bert-japanese-finetuned-sentiment\"\n",
        ")\n",
        "\n",
        "sentence1 = \"嬉しい\"\n",
        "sentence2 = \"悲しい\"\n",
        "result1 = classifier(sentence1)[0]\n",
        "result2 = classifier(sentence2)[0]\n",
        "print(f\"sentence1: {sentence1}, label: {result1['label']}, with score: {round(result1['score'], 4)}\")\n",
        "print(f\"sentence2: {sentence2}, label: {result2['label']}, with score: {round(result2['score'], 4)}\")"
      ],
      "metadata": {
        "id": "DWeuliAI7DWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- このコードでは、まずtransformersライブラリをインストールします。\n",
        "- 次に、日本語での感情分析に必要なfugashiとipadicライブラリをインストールします。\n",
        "- pipelineメソッドを使用して、感情分析を行います。\n",
        "- 日本語モデル\"koheiduck/bert-japanese-finetuned-sentiment\"をロードし、テキスト\"嬉しい\"と\"悲しい\"の感情分析を実行します。\n",
        "- 最終的には、各文章に対する感情分析の結果が出力されます。"
      ],
      "metadata": {
        "id": "IMy2vCI57GBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2モデルを使って文章生成をしよう"
      ],
      "metadata": {
        "id": "wwoWXwo09k4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**本章の目標**\n",
        "- GPT-2の理解\n",
        "- GPT-2の使い方の習得\n",
        "\n",
        "**GPT-2とは**\n",
        "- 自然言語処理モデルの一つ\n",
        "- 2019年にOpenAIによって発表\n",
        "- transformerをベースにしたテキスト生成モデル\n",
        "\n",
        "**GPT-2の特徴**\n",
        "- 逐次的な単語予測による文章生成\n",
        "- 日本語モデルも利用可能\n",
        "- 広告コピーからレシピ文まで幅広い文章生成に適用\n",
        "\n",
        "**GPT-2の利用方法**\n",
        "- transformersライブラリを利用\n",
        "- sentencepieceと併用して日本語モデルを読み込む\n",
        "\n",
        "**文章生成の実例**\n",
        "- 「自然言語処理とは」を起点にした文章生成\n",
        "- rinna社の日本語モデルを利用\n",
        "\n",
        "口語調の要約\n",
        "\n",
        "GPT-2っていうのは、文章を生成するためのモデルです。自然言語処理の分野で使われていて、OpenAIが開発しました。\n",
        "\n",
        "このモデルはtransformerを基にしていて、与えられたテキストに基づいて次の単語を予測するんです。日本語でも使えます。広告のコピーや、レシピの文章、いろんな文章を自動で作ってくれます。\n",
        "\n",
        "使い方は簡単で、transformersというライブラリを使います。日本語で使う場合は、sentencepieceっていうツールも一緒に使います。\n",
        "\n",
        "例えば、「自然言語処理とは」って言葉から文章を生成することもできます。rinna社の日本語モデルを使って、色んな文章を作ってみることが可能です。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g2Po00vp9mYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-2用のライブラリをインストール\n",
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "22pFz0GX-yqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "891ac7b1-3000-438c-b672-229a4f4fa792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n"
      ],
      "metadata": {
        "id": "njOtmRks_T_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the Japanese model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-small\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-small\")\n",
        "\n",
        "# Generate text\n",
        "input_text = \"自然言語処理とは,\"\n",
        "input = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, max_length=30, num_return_sequences=3)\n",
        "print(tokenizer.batch_decode(output))\n"
      ],
      "metadata": {
        "id": "gFLZyVON-pdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- このコードでは、まずGPT-2用のライブラリ、sentencepieceとtransformersをインストールします。\n",
        "- 次に、rinna社の日本語モデルを読み込みます。\n",
        "- `T5Tokenizer.from_pretrained` と `AutoModelForCausalLM.from_pretrained` を使って、モデルとトークナイザを準備します。\n",
        "- 入力されたテキスト\"自然言語処理とは,\"に基づいて、文章生成を行います。\n",
        "- `model.generate`メソッドで生成した文章を出力します。"
      ],
      "metadata": {
        "id": "TKkF2BXy-rSN"
      }
    }
  ]
}